<!DOCTYPE html>
<html lang="en-US">
<head>
  <title> PyTorch Ecosystem Day 2021</title>
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-52DXT37');</script>
	<!-- End Google Tag Manager -->
	<!-- Docsearch -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css">
	<script src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
	<!-- End Docsearch -->

	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="robots" content="max-image-preview:large">
<link rel="dns-prefetch" href="//s.w.org">
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.7.5"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}</style>
	<link rel="stylesheet" id="pytorch-classic-style-css" href="/wp-content/themes/pytorch/assets/css/index.css?ver=1" media="all">
<link rel="stylesheet" id="pytorch-blog-header-css" href="/wp-content/themes/pytorch/assets/css/blog-header.css?ver=1" media="all">
<link rel="stylesheet" id="pytorch-text-css" href="/wp-content/themes/pytorch/assets/css/text.css?ver=1" media="all">
<link rel="stylesheet" id="pytorch-youtube-video-css" href="/wp-content/themes/pytorch/assets/css/youtube-video.css?ver=1" media="all">
<link rel="stylesheet" id="pytorch-post-feed-css" href="/wp-content/themes/pytorch/assets/css/post-feed.css?ver=1" media="all">
<link rel="stylesheet" id="mediaelement-css" href="/wp-includes/js/mediaelement/mediaelementplayer-legacy.min.css?ver=4.2.16" media="all">
<link rel="stylesheet" id="wp-mediaelement-css" href="/wp-includes/js/mediaelement/wp-mediaelement.min.css?ver=5.7.5" media="all">
<link rel="stylesheet" id="ssp-search-css" href="/wp-content/plugins/simply-static-pro/assets/ssp-search.css?ver=1.1" media="all">
<link rel="https://api.w.org/" href="/wp-json/">
<link rel="alternate" type="application/json" href="/wp-json/wp/v2/events/311">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress 5.7.5">
<link rel="canonical" href="/events/pytorch-ecosystem-day-2021/">
<link rel="shortlink" href="/?p=311">
		<meta name="ssp-url" content="">
		<meta name="ssp-config-url" content="/wp-content/plugins/simply-static-pro/configs/">
		<style type="text/css">img#wpstats{display:none}</style>
		</head>

<body class="events-template-default single single-events postid-311">
    <a id="skip" href="#main" tabindex="0">Skip to content</a>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-52DXT37" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div>
	<header class="mainHeader">
		<div class="container">
			<div class="logo">
				<a href="/" aria-label="PyTorch Logo" tabindex="0">
					<svg role="img" width="109" height="27" viewbox="0 0 109 27" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M14.6172 5.92839L13.1892 7.32839C15.5692 9.66172 15.5692 13.4617 13.1892 15.7951C10.8092 18.1284 6.93316 18.1284 4.55316 15.7951C2.17316 13.4617 2.17316 9.66172 4.55316 7.32839L8.36116 3.59505L8.83716 3.06172V0.261719L3.05716 5.92839C-0.138844 9.06172 -0.138844 14.0617 3.05716 17.1951C6.25316 20.3284 11.3532 20.3284 14.5492 17.1951C17.8132 14.0617 17.8132 8.99505 14.6172 5.92839Z" fill="#DE3412"></path>
<ellipse cx="11.7618" cy="4.4612" rx="1.088" ry="1.06667" fill="#DE3412"></ellipse>
<path fill-rule="evenodd" clip-rule="evenodd" d="M26.5048 13.23H28.9435C33.3467 13.1625 36.1242 11.205 36.1242 7.29C36.1242 3.9825 33.8887 1.6875 29.1467 1.6875H24.6758V19.5075H26.5048V13.23ZM26.4371 3.375H29.079C32.4661 3.375 34.2274 4.7925 34.2274 7.29C34.2274 10.0575 32.3984 11.4075 29.0113 11.475L26.4371 11.5425V3.375Z" fill="#262626"></path>
<path d="M44.3887 19.4405L43.3048 22.2755C42.0855 25.448 40.8661 26.393 39.0371 26.393C38.021 26.393 37.2758 26.123 36.4629 25.7855L37.0048 24.1655C37.6145 24.503 38.2919 24.7055 39.0371 24.7055C40.0532 24.7055 40.7984 24.1655 41.7468 21.668L42.6274 19.373L37.5468 6.48047H39.4435L43.5758 17.2805L47.6403 6.48047H49.4693L44.3887 19.4405Z" fill="#262626"></path>
<path d="M55.5665 3.4425V19.575H53.7375V3.4425H47.4375V1.6875H61.8665V3.375H55.5665V3.4425Z" fill="#262626"></path>
<path fill-rule="evenodd" clip-rule="evenodd" d="M60.6465 13.0277C60.6465 17.2127 63.3562 19.9127 67.0142 19.9127C70.6723 19.9127 73.4497 17.1452 73.4497 12.9602C73.4497 8.7752 70.8078 6.0752 67.1497 6.0752C63.4239 6.0752 60.6465 8.8427 60.6465 13.0277ZM62.4746 12.9613C62.4746 9.92379 64.3036 7.69629 67.0811 7.69629C69.8585 7.69629 71.7553 9.85629 71.7553 13.0288C71.7553 16.0663 69.9262 18.2938 67.1488 18.2938C64.3714 18.2938 62.4746 16.1338 62.4746 12.9613Z" fill="#262626"></path>
<path d="M77.9879 19.5731H76.2266V6.47813L77.9879 6.14062V8.90812C78.8685 7.22062 80.1556 6.14062 81.8491 6.14062C82.7298 6.14062 83.5427 6.41063 84.1524 6.74813L83.6782 8.43563C83.1362 8.09813 82.3911 7.89562 81.6459 7.89562C80.2911 7.89562 79.004 8.90813 77.9201 11.2706V19.5731H77.9879Z" fill="#262626"></path>
<path d="M91.1308 19.9127C87.2018 19.9127 84.6953 17.0777 84.6953 13.0277C84.6953 8.9102 87.405 6.0752 91.1308 6.0752C92.7566 6.0752 94.1114 6.4802 95.2631 7.2227L94.7889 8.8427C93.7727 8.1677 92.5534 7.7627 91.1308 7.7627C88.2856 7.7627 86.5243 9.8552 86.5243 12.9602C86.5243 16.1327 88.4211 18.2252 91.1985 18.2252C92.4856 18.2252 93.8405 17.8202 94.8566 17.1452L95.1953 18.7652C94.0437 19.5077 92.6211 19.9127 91.1308 19.9127Z" fill="#262626"></path>
<path d="M106.238 19.575V11.1375C106.238 8.8425 105.29 7.83 103.461 7.83C101.97 7.83 100.48 8.5725 99.3961 9.72V19.6425H97.6348V0.3375L99.3961 0C99.3961 0 99.3961 8.1675 99.3961 8.235C100.751 6.885 102.512 6.1425 103.935 6.1425C106.509 6.1425 108.067 7.7625 108.067 10.5975V19.575H106.238Z" fill="#262626"></path>
</svg>
				</a>
			</div>
			<div class="content-container">
				<ul class="mainNav">
					<li class="mainItem search" key="search">
						<button id="search-icon" aria-label="search">
							<svg role="img" width="17" height="16" viewbox="0 0 17 16" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M11.7461 7C11.7461 9.48528 9.73137 11.5 7.24609 11.5C4.76081 11.5 2.74609 9.48528 2.74609 7C2.74609 4.51472 4.76081 2.5 7.24609 2.5C9.73137 2.5 11.7461 4.51472 11.7461 7ZM10.9253 11.7399C9.90931 12.5297 8.63263 13 7.24609 13C3.93239 13 1.24609 10.3137 1.24609 7C1.24609 3.68629 3.93239 1 7.24609 1C10.5598 1 13.2461 3.68629 13.2461 7C13.2461 8.38653 12.7758 9.66322 11.986 10.6792L15.2764 13.9697L14.2158 15.0303L10.9253 11.7399Z" fill="#262626"></path>
</svg>
						</button>
						<div class="search-border">
							<div id="inner-search-icon">
								<svg role="img" width="17" height="16" viewbox="0 0 17 16" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M11.7461 7C11.7461 9.48528 9.73137 11.5 7.24609 11.5C4.76081 11.5 2.74609 9.48528 2.74609 7C2.74609 4.51472 4.76081 2.5 7.24609 2.5C9.73137 2.5 11.7461 4.51472 11.7461 7ZM10.9253 11.7399C9.90931 12.5297 8.63263 13 7.24609 13C3.93239 13 1.24609 10.3137 1.24609 7C1.24609 3.68629 3.93239 1 7.24609 1C10.5598 1 13.2461 3.68629 13.2461 7C13.2461 8.38653 12.7758 9.66322 11.986 10.6792L15.2764 13.9697L14.2158 15.0303L10.9253 11.7399Z" fill="#262626"></path>
</svg>
							</div>
								<input id="input" type="text" placeholder="Search" title="Search" autocomplete="off" role="combobox" aria-autocomplete="list" aria-expanded="false" aria-label="search input">
							
							<div id="close-search">X</div>
						</div>
					</li>
					<li class="mainItem menuIcon">
						<span class="hamburger"><svg role="img" width="22" height="18" viewbox="0 0 22 18" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M21.5 2.125H0.5V0.625H21.5V2.125ZM21.5 17.625H0.5V16.125H21.5V17.625ZM0.5 9.75H21.5V8.25H0.5V9.75Z" fill="black"></path>
</svg>
</span>
						<span class="close"><svg role="img" width="22" height="22" viewbox="0 0 22 22" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M11.0001 12.0607L20.2197 21.2803L21.2804 20.2197L12.0607 11L21.2804 1.78033L20.2197 0.719675L11.0001 9.93934L1.78039 0.719673L0.719727 1.78033L9.9394 11L0.719727 20.2197L1.78039 21.2803L11.0001 12.0607Z" fill="black"></path>
</svg>
</span>
					</li>
					<li class="navItems">
						<ul class="navItemsContainer">
																														<li class="mainItem home-menu">
									<a class="parentTitle" href="/" target="">
										<span>
											Home										</span>
									</a>
																	</li>
																							<li class="mainItem install-menu">
									<a class="parentTitle" href="/install/install-overview/" target="">
										<span>
											Install										</span>
									</a>
																			<div class="subitems-container">
											<div class="subitems-wrapper">
												<ul class="subItems">
																											<li class="subItem ">
															<a class="link" href="/install/install-overview/" target="">
																<p class="title">Overview</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/install/install-locally/" target="">
																<p class="title">Install Locally</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/install/install-mobile/" target="">
																<p class="title">Install Mobile</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/install/start-with-cloud-partners/" target="">
																<p class="title">Start with Cloud Partners</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/install/previous-versions/" target="">
																<p class="title">Previous PyTorch Versions</p>
																														</a>
														</li>
																									</ul>
											</div>
										</div>
																	</li>
																							<li class="mainItem features-menu">
									<a class="parentTitle" href="/features/" target="">
										<span>
											Features										</span>
									</a>
																	</li>
																							<li class="mainItem resources-menu">
									<a class="parentTitle" href="/resources/overview/" target="">
										<span>
											Resources										</span>
									</a>
																			<div class="subitems-container">
											<div class="subitems-wrapper">
												<ul class="subItems">
																											<li class="subItem ">
															<a class="link" href="/resources/overview/" target="">
																<p class="title">Overview</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/resources/blog/" target="">
																<p class="title">Blog</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/resources/education/" target="">
																<p class="title">Education</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/resources/localized-docs-tutorials/" target="">
																<p class="title">Localized Docs &#038; Tutorials</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/resources/enterprise-program/" target="">
																<p class="title">Enterprise Program</p>
																														</a>
														</li>
																									</ul>
											</div>
										</div>
																	</li>
																							<li class="mainItem docs-menu">
									<a class="parentTitle" href="https://pytorch.org/docs/stable/index.html" target="_blank">
										<span>
											Docs										</span>
									</a>
																	</li>
																							<li class="mainItem tutorials-menu">
									<a class="parentTitle" href="https://pytorch.org/tutorials/" target="_blank">
										<span>
											Tutorials										</span>
									</a>
																	</li>
																							<li class="mainItem community-menu">
									<a class="parentTitle" href="/community/overview/" target="">
										<span>
											Community										</span>
									</a>
																			<div class="subitems-container">
											<div class="subitems-wrapper">
												<ul class="subItems">
																											<li class="subItem ">
															<a class="link" href="/community/overview/" target="">
																<p class="title">Overview</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/community/case-studies/" target="">
																<p class="title">Case Studies</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/community/ecosystem-tools/" target="">
																<p class="title">Ecosystem Tools</p>
																														</a>
														</li>
																											<li class="subItem ">
															<a class="link" href="/community/events/" target="">
																<p class="title">Events</p>
																														</a>
														</li>
																									</ul>
											</div>
										</div>
																	</li>
														<li class="github mainItem">
								<div class="github-wrapper">
									<a href="https://github.com/pytorch/pytorch" target="_blank" class="parentTitle" aria-label="See Pytorch on GitHub">
										<svg role="img" width="25" height="24" viewbox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M23.0227 6.33146C21.9311 4.51257 20.4504 3.07256 18.5803 2.01109C16.7099 0.949571 14.6679 0.418945 12.453 0.418945C10.2384 0.418945 8.19578 0.949733 6.32575 2.01109C4.45545 3.07251 2.97484 4.51257 1.88325 6.33146C0.791831 8.15029 0.246094 10.1365 0.246094 12.2899C0.246094 14.8767 1.02214 17.2029 2.57462 19.2689C4.12694 21.3351 6.1323 22.7648 8.59054 23.5582C8.87669 23.6099 9.08851 23.5736 9.22624 23.4502C9.36403 23.3266 9.43284 23.1719 9.43284 22.9866C9.43284 22.9557 9.43011 22.6776 9.42482 22.152C9.41936 21.6263 9.4168 21.1677 9.4168 20.7765L9.05121 20.838C8.81812 20.8795 8.52407 20.8971 8.16906 20.8921C7.81422 20.8873 7.44584 20.8511 7.06445 20.7839C6.68288 20.7172 6.32798 20.5627 5.99947 20.3205C5.67113 20.0783 5.43803 19.7614 5.30025 19.37L5.14131 19.0143C5.03537 18.7775 4.86858 18.5145 4.64072 18.2261C4.41286 17.9375 4.18244 17.7418 3.94935 17.6388L3.83806 17.5613C3.76391 17.5098 3.6951 17.4477 3.63147 17.3757C3.5679 17.3036 3.5203 17.2315 3.48851 17.1593C3.45667 17.087 3.48305 17.0277 3.56795 16.9812C3.65285 16.9347 3.80628 16.9121 4.0289 16.9121L4.34667 16.9583C4.55861 16.9996 4.82076 17.123 5.13346 17.3292C5.44599 17.5353 5.70291 17.8032 5.90427 18.1328C6.14811 18.5554 6.44188 18.8774 6.78643 19.099C7.13069 19.3206 7.4778 19.4312 7.82741 19.4312C8.17702 19.4312 8.47898 19.4054 8.73339 19.3542C8.98753 19.3026 9.22596 19.2252 9.44859 19.1222C9.54395 18.4315 9.8036 17.9008 10.2273 17.5299C9.62339 17.4682 9.08044 17.3752 8.59817 17.2516C8.11617 17.1279 7.61809 16.927 7.10425 16.6485C6.59013 16.3704 6.16364 16.025 5.82466 15.613C5.48563 15.2008 5.20739 14.6596 4.99033 13.99C4.77316 13.3201 4.66455 12.5473 4.66455 11.6714C4.66455 10.4243 5.08319 9.36302 5.92031 8.48704C5.52816 7.54944 5.56518 6.49837 6.03148 5.33393C6.33878 5.24108 6.7945 5.31076 7.39841 5.54253C8.00244 5.77441 8.44468 5.97305 8.7256 6.13775C9.00651 6.30238 9.23159 6.4419 9.40116 6.55506C10.3868 6.28723 11.404 6.15328 12.4529 6.15328C13.5018 6.15328 14.5192 6.28723 15.5049 6.55506L16.1089 6.18425C16.5219 5.93683 17.0096 5.71009 17.5709 5.50398C18.1325 5.29798 18.562 5.24124 18.8588 5.33409C19.3354 6.49859 19.3779 7.54961 18.9857 8.4872C19.8227 9.36319 20.2415 10.4247 20.2415 11.6715C20.2415 12.5474 20.1325 13.3227 19.9157 13.9977C19.6986 14.6729 19.4179 15.2135 19.0737 15.6208C18.729 16.028 18.2998 16.3706 17.786 16.6487C17.272 16.927 16.7738 17.1278 16.2918 17.2516C15.8096 17.3754 15.2666 17.4684 14.6627 17.5302C15.2135 17.9937 15.4889 18.7254 15.4889 19.725V22.9862C15.4889 23.1715 15.5552 23.3261 15.6878 23.4497C15.8202 23.5731 16.0294 23.6095 16.3155 23.5578C18.7741 22.7644 20.7795 21.3347 22.3317 19.2685C23.8838 17.2024 24.6602 14.8763 24.6602 12.2895C24.6596 10.1363 24.1136 8.15029 23.0227 6.33146Z" fill="#262626"></path>
</svg>
									</a>
								</div>
							</li>
						</ul>
						
<div class="navItems-followContainer followMenu ">
			<div class="mainItem">Follow Us</div>
		<ul class="follow-list">
					<li class="followItem">
				<a aria-label="follow Menu  icon-twitter" href="https://twitter.com/pytorch" target="_blank" class=" icon-twitter">
					Twitter				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-facebook" href="https://www.facebook.com/pytorch" target="_blank" class=" icon-facebook">
					Facebook				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-youtube" href="https://www.youtube.com/pytorch" target="_blank" class=" icon-youtube">
					YouTube				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-linkedin" href="https://www.linkedin.com/company/pytorch" target="_blank" class=" icon-linkedin">
					Linkedin				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-medium" href="https://medium.com/pytorch" target="_blank" class=" icon-medium">
					Medium				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-github" href="https://github.com/pytorch/pytorch" target="_blank" class=" icon-github">
					GitHub				</a>
			</li>
			</ul>
</div>
					</li>
				</ul>
			</div>
		</div>
	</header>

	<main id="main" class="site-main" role="main">


<div class="Breadcrumbs">
	<div class="container breadcrumbs-lined">
		<ul class="list-container">
						<li class="breadcrumb-item breadcrumb-active">
				<a class="active-item" href="/community/events/">Events</a>
			</li>
						<li class="breadcrumb-item">
				PyTorch Ecosystem Day 2021			</li>
		</ul>
	</div>
</div>

<div class="BlogHeader ">
	<div class="container">
		<h2 class="title"></h2>

		<div class="banner">
			
<img class="Image " src="/wp-content/uploads/2021/09/pytorch_ecosystem_day_2021.jpeg" alt="">
		</div>

		<div class="foot">
			<div class="byline">
								<span class="date"></span>
				<span class="author"></span>
			</div>

			
<div class="Share ">
  
  <div class="share-icons">
    
      </div>
</div>
		</div>


	</div>
</div>




<div class="TextBlock is-style-medium-text TextBlock padding-size-small padding-size-small">
	<div class="container">
				<p class="rich-text">Thank you to the incredible PyTorch Community for making the first ever PyTorch Ecosystem Day a success! Ecosystem Day was hosted on Gather.Town utilizing an auditorium, exhibition hall, and breakout rooms for partners to reserve for talks, demos, or tutorials. In order to cater to the global community, the event held two sessions: a morning session from 8am PT &#8211; 1pm PT and an evening session from 3:00pm -7:00pm PT. The day was filled with discussions on new developments, trends and challenges showcased through 71 posters, 32 breakout sessions and 6 keynote speakers.<br><br>Special thanks to our 6 keynote speakers: Piotr Bialecki, Ritchie Ng, Miquel Farré, Joe Spisak, Geeta Chauhan, and Suraj Subramanian. You can find the opening talks here:</p>
	</div>
</div>


<div class="YoutubeVideo YoutubeVideo">
	<div class="container">
		<div class="video-wrapper">
			<iframe src="https://www.youtube.com/embed/MYE01-XaSZA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
			</iframe>
		</div>
	</div>
</div>



<div class="YoutubeVideo YoutubeVideo">
	<div class="container">
		<div class="video-wrapper">
			<iframe src="https://www.youtube.com/embed/CjU_6OaYKpw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
			</iframe>
		</div>
	</div>
</div>



<div class="PostFeed row-background-undefined has-filters block-21930c15-7778-4fa0-9c3c-b80eeaa7e832 block-21930c15-7778-4fa0-9c3c-b80eeaa7e832">
			<div class="filter-container-mobile">
				<section class="checkbox-filter-container-dropdown ">
		<h4 class="filter-title dropdown-clickable"><span>Filter by</span></h4>
		<div class="checkbox-filter-wrapper-dropdown">
										<div class="checkbox-filter-type">
					<div class="dropdown-sub-title dropdown-clickable ">Tags</div>
					<div class="checkbox-filter-categories">
																					
	<label class="Checkbox  smaller">
	compiler	<input class="checkbox-input" type="checkbox" name="compiler" value="tags-compiler">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	compiler-transform-production	<input class="checkbox-input" type="checkbox" name="compiler-transform-production" value="tags-compiler-transform-production">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	database-ai-accelerators	<input class="checkbox-input" type="checkbox" name="database-ai-accelerators" value="tags-database-ai-accelerators">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	distributed-training	<input class="checkbox-input" type="checkbox" name="distributed-training" value="tags-distributed-training">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	frontend-experiment-manager	<input class="checkbox-input" type="checkbox" name="frontend-experiment-manager" value="tags-frontend-experiment-manager">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	medical-healthcare	<input class="checkbox-input" type="checkbox" name="medical-healthcare" value="tags-medical-healthcare">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	nlp-multimodal	<input class="checkbox-input" type="checkbox" name="nlp-multimodal" value="tags-nlp-multimodal">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	ops-tools	<input class="checkbox-input" type="checkbox" name="ops-tools" value="tags-ops-tools">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	performance-profiler	<input class="checkbox-input" type="checkbox" name="performance-profiler" value="tags-performance-profiler">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	platform	<input class="checkbox-input" type="checkbox" name="platform" value="tags-platform">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	platforms-ops-tools	<input class="checkbox-input" type="checkbox" name="platforms-ops-tools" value="tags-platforms-ops-tools">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	production	<input class="checkbox-input" type="checkbox" name="production" value="tags-production">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	rl-time-series	<input class="checkbox-input" type="checkbox" name="rl-time-series" value="tags-rl-time-series">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	transform	<input class="checkbox-input" type="checkbox" name="transform" value="tags-transform">
	<span class="checkmark"></span>
</label>
																												
	<label class="Checkbox  smaller">
	vision	<input class="checkbox-input" type="checkbox" name="vision" value="tags-vision">
	<span class="checkmark"></span>
</label>
																		</div>
				</div>
								</div>
	</section>
		</div>
				<h2 class="heading">Posters</h2>
		<div class="container">
				<div class="filter-container">
				<section class="checkbox-filter-container  ">
		<h4 class="filter-title">Filter by</h4>
							<div class="checkbox-filter-type">
				<div class="taxonomy-title">Tags</div>
				<div class="checkbox-filter-categories">
																		
	<label class="Checkbox  smaller">
	compiler	<input class="checkbox-input" type="checkbox" name="compiler" value="tags-compiler">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	compiler-transform-production	<input class="checkbox-input" type="checkbox" name="compiler-transform-production" value="tags-compiler-transform-production">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	database-ai-accelerators	<input class="checkbox-input" type="checkbox" name="database-ai-accelerators" value="tags-database-ai-accelerators">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	distributed-training	<input class="checkbox-input" type="checkbox" name="distributed-training" value="tags-distributed-training">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	frontend-experiment-manager	<input class="checkbox-input" type="checkbox" name="frontend-experiment-manager" value="tags-frontend-experiment-manager">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	medical-healthcare	<input class="checkbox-input" type="checkbox" name="medical-healthcare" value="tags-medical-healthcare">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	nlp-multimodal	<input class="checkbox-input" type="checkbox" name="nlp-multimodal" value="tags-nlp-multimodal">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	ops-tools	<input class="checkbox-input" type="checkbox" name="ops-tools" value="tags-ops-tools">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	performance-profiler	<input class="checkbox-input" type="checkbox" name="performance-profiler" value="tags-performance-profiler">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	platform	<input class="checkbox-input" type="checkbox" name="platform" value="tags-platform">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	platforms-ops-tools	<input class="checkbox-input" type="checkbox" name="platforms-ops-tools" value="tags-platforms-ops-tools">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	production	<input class="checkbox-input" type="checkbox" name="production" value="tags-production">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	rl-time-series	<input class="checkbox-input" type="checkbox" name="rl-time-series" value="tags-rl-time-series">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	transform	<input class="checkbox-input" type="checkbox" name="transform" value="tags-transform">
	<span class="checkmark"></span>
</label>
																								
	<label class="Checkbox  smaller">
	vision	<input class="checkbox-input" type="checkbox" name="vision" value="tags-vision">
	<span class="checkmark"></span>
</label>
															</div>
			</div>
					</section>
		</div>
				<section class="article-container">
							<div class="header">
											<p class="post-count">
							<span class="count">73</span>
							<span>posts</span>
						</p>
																<div class="SortDropdown-wrapper">
	<select class="SortDropdown dropdown" name="sort" data-container-class="block-21930c15-7778-4fa0-9c3c-b80eeaa7e832" data-item-class="FeedCard">
		<option value="" disabled selected>Sort</option>
		<option value="latest">Date &mdash; Latest</option>
		<option value="oldest">Date &mdash; Oldest</option>
	</select>
</div>
									</div>
						<div class="feed">
									
<article class="FeedCard  " data-filter-ids="tags-ops-tools tags-platform">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/02/K1-1.png" target="_blank" aria-label="poster Image Bring quantum machine learning to PyTorch with PennyLane">
				
<img class="Image image" src="/wp-content/uploads/2021/02/K1-1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Bring quantum machine learning to PyTorch with PennyLane</h3>

		<p class="description">PennyLane allows you to train quantum circuits just like neural networks!, This poster showcases how PennyLane can be interfaced with PyTorch to enable training of quantum and hybrid machine learning models. The outputs of a quantum circuit are provided as a Torch tensor with a defined gradient. We highlight how this functionality can be used to explore new paradigms in machine learning, including the use of hybrid models for transfer learning.<br><br><a href="http://www.pennylane.ai/" target="_blank">http://www.pennylane.ai</a></p>

		<div class="card-footer">
							<p class="author">by Josh Izaac, Thomas Bromley</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard  " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A4.png" target="_blank" aria-label="poster Image PyTorch development in VS Code">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch development in VS Code</h3>

		<p class="description">Visual Studio Code, a free cross-platform lightweight code editor, has become the most popular among Python developers for both web and machine learning projects. We will be walking you through an end to end PyTorch project to showcase what VS Code has a lot to offer to PyTorch developers to boost their productivity. Firstly, get your PyTorch project quickly up and running with VS Code&#8217;s environment/dependency management and built-in Jupyter Notebook support. Secondly, breeze through coding with help from our AI-powered IntelliSense. When it&#8217;s time to run your code, use the built-in Tensorboard integration to monitor your training along with the integrated PyTorch profiler to analyze and debug your code. Once you&#8217;re ready for the cloud, VS Code has Azure service integration to allow you to scale your model training and deployment, along with deployment. Combing the power of the code editor with easy access to the Azure services, VS Code can be the one-stop shop for any developers looking to build machine learning models with PyTorch.<br><br><a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/">https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</a></p>

		<div class="card-footer">
							<p class="author">by Jeffrey Mew</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard  " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A5.png" target="_blank" aria-label="poster Image Upcoming features in TorchScript">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Upcoming features in TorchScript</h3>

		<p class="description">TorchScript is the bridge between PyTorch&#8217;s flexible eager mode to more deterministic and performant graph mode suitable for production deployment. As part of PyTorch 1.9 release, TorchScript will launch a few features that we&#8217;d like to share with you earlier, including a) a new formal language specification that defines the exact subset of Python/PyTorch features supported in TorchScript; b) Profile-Directed Typing that reduces the burden of converting a loosely-typed eager model into a strictly-typed TorchScript model; c) A TorchScript profiler that can shed light on performance characteristics of TorchScript model. We are constantly making improvements to make TorchScript easier to use and more performant.<br><br><a href="http://fb.me/torchscript" target="_blank">http://fb.me/torchscript</a></p>

		<div class="card-footer">
							<p class="author">by Yanan Cao, Harry Kim, Jason Ansel</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard  " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Quantization-Aware Training with Brevitas</h3>

		<p class="description">Brevitas is an open-source PyTorch library for quantization-aware training. Thanks to its flexible design at multiple levels of abstraction, Brevitas generalizes the typical uniform affine quantization paradigm adopted in the deep learning community under a common set of unified APIs. Brevitas provides a platform to both ML practitioners and researchers to either apply built-in state-of-the-art techniques in training for reduced-precision inference, or to implement novel quantization-aware training algorithms. Users can target supported inference toolchains, such as onnxruntime, TVM, Vitis AI, FINN or PyTorch itself, or experiment with hypothetical target hardware platforms. In particular, when combined with the flexibility of Xilinx FPGAs through the FINN toolchain, Brevitas supports the co-design of novel hardware building blocks in a machine-learning driven fashion. Within Xilinx, Brevitas has been adopted by various research projects concerning quantized neural networks, as well as in large scale deployments targeting custom programmable logic accelerators.<br><br><a href="https://github.com/Xilinx/brevitas/" target="_blank">https://github.com/Xilinx/brevitas/</a></p>

		<div class="card-footer">
							<p class="author">by Alessandro Pappalardo</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard  " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/B5.png" target="_blank" aria-label="poster Image PyTorch Quantization: FX Graph Mode Quantization">
				
<img class="Image image" src="/wp-content/uploads/2021/12/B5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch Quantization: FX Graph Mode Quantization</h3>

		<p class="description">Quantization is a common model optimization technique to speedup runtime of a model by upto 4x, with a possible slight loss of accuracy. Currently, PyTorch support Eager Mode Quantization. FX Graph Mode Quantization improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process. To use FX Graph Mode Quantization, one might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx).<br><br><a href="https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization">https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization</a></p>

		<div class="card-footer">
							<p class="author">by Jerry Zhang, Vasiliy Kuznetsov, Raghuraman Krishnamoorthi</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C4.png" target="_blank" aria-label="poster Image Accelerate deployment of deep learning models in production with Amazon EC2 Inf1 and TorchServe containers">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Accelerate deployment of deep learning models in production with Amazon EC2 Inf1 and TorchServe containers</h3>

		<p class="description">Deep learning models can have game-changing impact on machine learning applications. However, deploying and managing deep learning models in production is complex and requires considerable engineering effort &#8211; from building custom inferencing APIs and scaling prediction services, to securing applications, while still leveraging the latest ML frameworks and hardware technology. Amazon EC2 Inf1 instances powered by AWS Inferentia deliver the highest performance and lowest cost machine learning inference in the cloud. Developers can deploy their deep-learning models to Inf1 instances using the AWS Neuron SDK that is natively integrated with PyTorch. Attend this poster session to learn how you can optimize and accelerate the deployment of your deep learning models in production using Inf1 instances and TorchServe containers. You will learn how to deploy TorchScript models on Inf1 and optimize your models with minimal code changes with features such as NeuronCore Groups and NeuronCore Pipeline, to meet your throughput and latency requirements. You can directly integrate these model level optimizations into the inference endpoint using TorchServe. We will also deep dive into how we optimized performance of a natural language processing endpoint and showcase the workflow for deploying the optimized model using TorchServe containers on Amazon ECS.<br><br><a href="https://bit.ly/3mQVowk" target="_blank">https://bit.ly/3mQVowk</a></p>

		<div class="card-footer">
							<p class="author">by Fabio Nonato</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C5.png" target="_blank" aria-label="poster Image Torch.fx">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Torch.fx</h3>

		<p class="description">FX is a toolkit for writing Python-to-Python transforms over PyTorch code. FX consists of three parts: &gt; Symbolic Tracing – a method to extract a representation of the program by running it with &#8220;proxy&#8221; values. &gt; Graph-based Transformations – FX provides an easy-to-use Python-based Graph API for manipulating the code. &gt; Python code generation – FX generates valid Python code from graphs and turns that code into executable Python `nn.Module` instances.<br><br><a href="https://pytorch.org/docs/stable/fx.html">https://pytorch.org/docs/stable/fx.html</a></p>

		<div class="card-footer">
							<p class="author">by James Reed, Zachary DeVito, Ansley Ussery, Horace He, Michael Suo</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D4.png" target="_blank" aria-label="poster Image AI Model Efficiency Toolkit (AIMET)">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">AI Model Efficiency Toolkit (AIMET)</h3>

		<p class="description">AI is revolutionizing industries, products, and core capabilities by delivering dramatically enhanced experiences. However, the deep neural networks of today use too much memory, compute, and energy. To make AI truly ubiquitous, it needs to run on the end device within a tight power and thermal budget. Quantization and compression help address these issues. In this tutorial, we&#8217;ll discuss: The existing quantization and compression challenges Our research in novel quantization and compression techniques to overcome these challenges How developers and researchers can implement these techniques through the AI Model Efficiency Toolkit</p>

		<div class="card-footer">
							<p class="author">by Abhijit Khobare, Murali Akula, Tijmen Blankevoort, Harshita Mangal, Frank Mayer, Sangeetha Marshathalli Siddegowda, Chirag Patel, Vinay Garg, Markus Nagel</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-database-ai-accelerators">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H8.png" target="_blank" aria-label="poster Image Pytorch via SQL commands: A flexible, modular AutoML framework that democratizes ML for database users">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Pytorch via SQL commands: A flexible, modular AutoML framework that democratizes ML for database users</h3>

		<p class="description">Pytorch enables building models with complex inputs and outputs, including time-series data, text and audiovisual data. However, such models require expertise and time to build, often spent on tedious tasks like cleaning the data or transforming it into a format that is expected by the models. Thus, pre-trained models are often used as-is when a researcher wants to experiment only with a specific facet of a problem. See, as examples, FastAI&#8217;s work into optimizers, schedulers, and gradual training through pre-trained residual models, or NLP projects with Hugging Face models as their backbone. We think that, for many of these problems, we can automatically generate a &#8220;good enough&#8221; model and data-processing pipeline from just the raw data and the endpoint. To address this situation, we are developing MindsDB, an open-source, PyTorch-based ML platform that works inside databases via SQL commands. It is built with a modular approach, and in this talk we are going to focus on Lightwood, the stand-alone core component that performs machine learning automation on top of the PyTorch framework. Lightwood automates model building into 5 stages: (1) classifying each feature into a &#8220;data type&#8221;, (2) running statistical analyses on each column of a dataset, (3) fitting multiple models to normalize, tokenize, and generate embeddings for each feature, (4) deploying the embeddings to fit a final estimator, and (5) running an analysis on the final ensemble to evaluate it and generate a confidence model. It can generate quick &#8220;baseline&#8221; models to benchmark performance for any custom encoder representation of a data type and can also serve as scaffolding for investigating new hypotheses (architectures, optimizers, loss-functions, hyperparameters, etc). We aim to present our benchmarks covering wide swaths of problem types and illustrate how Lightwood can be useful for researchers and engineers through a hands-on demo.<br><br><a href="https://mindsdb.com/" target="_blank">https://mindsdb.com</a></p>

		<div class="card-footer">
							<p class="author">by Natasha Seelam, Patricio Cerda-Mardini, Cosmo Jenytin, Jorge Torres</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-database-ai-accelerators">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J8.png" target="_blank" aria-label="poster Image PyTorch on Supercomputers Simulations and AI at Scale with SmartSim">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch on Supercomputers Simulations and AI at Scale with SmartSim</h3>

		<p class="description">SmartSim is an open source library dedicated to enabling online analysis and Machine Learning (ML) for traditional High Performance Computing (HPC) simulations. Clients are provided in common HPC simulation languages, C/C++/Fortran, that enable simulations to perform inference requests in parallel on large HPC systems. SmartSim utilizes the Redis ecosystem to host and serve PyTorch models alongside simulations. We present a use case of SmartSim where a global ocean simulation, used in climate modeling, is augmented with a PyTorch model to resolve quantities of eddy kinetic energy within the simulation.<br><br><a href="https://github.com/CrayLabs/SmartSim" target="_blank">https://github.com/CrayLabs/SmartSim</a></p>

		<div class="card-footer">
							<p class="author">by Sam Partee , Alessandro Rigazzi, Mathew Ellis, Benjamin Rob</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-database-ai-accelerators">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I8.png" target="_blank" aria-label="poster Image Model agnostic confidence estimation with conformal predictors for AutoML">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Model agnostic confidence estimation with conformal predictors for AutoML</h3>

		<p class="description">Many domains leverage the extraordinary predictive performance of machine learning algorithms. However, there is an increasing need for transparency of these models in order to justify deploying them in applied settings. Developing trustworthy models is a great challenge, as they are usually optimized for accuracy, relegating the fit between the true and predicted distributions to the background [1]. This concept of obtaining predicted probability estimates that match the true likelihood is also known as calibration. Contemporary ML models generally exhibit poor calibration. There are several methods that aim at producing calibrated ML models [2, 3]. Inductive conformal prediction (ICP) is a simple yet powerful framework to achieve this, offering strong guarantees about the error rates of any machine learning model [4]. ICP provides confidence scores and turns any point prediction into a prediction region through nonconformity measures, which indicate the degree of inherent strangeness a data point presents when compared to a calibration data split. In this work, we discuss the integration of ICP with MindsDB &#8211;an open source AutoML framework&#8211; successfully replacing its existing quantile loss approach for confidence estimation capabilities. Our contribution is threefold. First, we present a study on the effect of a &#8220;self-aware&#8221; neural network normalizer in the width of predicted region sizes (also known as efficiency) when compared to an unnormalized baseline. Our benchmarks consider results for over 30 datasets of varied domains with both categorical and numerical targets. Second, we propose an algorithm to dynamically determine the confidence level based on a target size for the predicted region, effectively prioritizing efficiency over a minimum error rate. Finally, we showcase the results of a nonconformity measure specifically tailored for small datasets. References: [1] Guo, C., Pleiss, G., Sun, Y., &amp; Weinberger, K.Q. (2017). On Calibration of Modern Neural Networks. ArXiv, abs/1706.04599. [2] Naeini, M., Cooper, G., &amp; Hauskrecht, M. (2015). Obtaining Well Calibrated Probabilities Using Bayesian Binning. Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 2015, 2901-2907 . [3] Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., &amp; Wilson, A. (2019). A Simple Baseline for Bayesian Uncertainty in Deep Learning. NeurIPS. [4] Papadopoulos, H., Vovk, V., &amp; Gammerman, A. (2007). Conformal Prediction with Neural Networks. 19th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2007), 2, 388-395.<br><br><a href="https://mindsdb.com/" target="_blank">https://mindsdb.com</a></p>

		<div class="card-footer">
							<p class="author">by Patricio Cerda-Mardini, Natasha Seelam</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-database-ai-accelerators">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/K8.png" target="_blank" aria-label="poster Image Enabling PyTorch on AMD Instinct™ GPUs with the AMD ROCm™ Open Software Platform">
				
<img class="Image image" src="/wp-content/uploads/2021/12/K8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Enabling PyTorch on AMD Instinct™ GPUs with the AMD ROCm™ Open Software Platform</h3>

		<p class="description">AMD Instinct GPUs are enabled with the upstream PyTorch repository via the ROCm open software platform. Now users can also easily download the installable Python package, built from the upstream PyTorch repository and hosted on pytorch.org. Notably, it includes support for distributed training across multiple GPUs and supports accelerated mixed precision training. AMD also provides hardware support for the PyTorch community build to help develop and maintain new features. This poster will highlight some of the work that has gone into enabling PyTorch support.<br><br><a href="https://pytorch.org/ecosystem/pted/www.amd.com/rocm">www.amd.com/rocm</a></p>

		<div class="card-footer">
							<p class="author">by Derek Bouius</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/E1.png" target="_blank" aria-label="poster Image DeepSpeed: Shattering barriers of deep learning speed &#038; scale">
				
<img class="Image image" src="/wp-content/uploads/2021/12/E1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">DeepSpeed: Shattering barriers of deep learning speed &#038; scale</h3>

		<p class="description">In the poster (and a talk during the breakout session), we will present three aspects of DeepSpeed (https://github.com/microsoft/DeepSpeed), a deep learning optimization library based on PyTorch framework: 1) How we overcome the GPU memory barrier by ZeRO-powered data parallelism. 2) How we overcome the network bandwidth barrier by 1-bit Adam and 1-bit Lamb compressed optimization algorithms. 3) How we overcome the usability barrier by integration with Azure ML, HuggingFace, and PyTorch Lightning.</p>

		<div class="card-footer">
							<p class="author">by DeepSpeed Team Microsoft Corporation</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Dask PyTorch DDP: A new library bringing Dask parallelization to PyTorch training</h3>

		<p class="description">We have developed a library that helps simplify the task of multi-machine parallel training for PyTorch models, bringing together the power of PyTorch DDP with Dask for parallelism on GPUs. Our poster describes the library and its core function, and demonstrates how the multi-machine training process works in practice.<br><br><a href="https://github.com/saturncloud/dask-pytorch-ddp" target="_blank">https://github.com/saturncloud/dask-pytorch-ddp</a></p>

		<div class="card-footer">
							<p class="author">by Stephanie Kirmer, Hugo Shi</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/E3.png" target="_blank" aria-label="poster Image Optimising Physics Informed Neural Networks.">
				
<img class="Image image" src="/wp-content/uploads/2021/12/E3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Optimising Physics Informed Neural Networks.</h3>

		<p class="description">Solving PDEs using Neural Networks are often ardently laborious as it requires training towards a well-defined solution, i.e. global minima for a network architecture &#8211; objective function combination. For a family of complex PDEs, Physics Informed neural networks won&#8217;t offer much in comparison to traditional numerical methods as their global minima becomes more and more intractable. We propose a modified approach that hinges on continual and parametrised learning that can create more general PINNs that can solve for a variety of PDE scenarios rather than solving for a well-defined case. We believe that this brings Neural Network based PDE solvers in comparison to numerical solvers.</p>

		<div class="card-footer">
							<p class="author">by Vignesh Gopakumar</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/F1.png" target="_blank" aria-label="poster Image FairScale-A general purpose modular PyTorch library for high performance and large scale training">
				
<img class="Image image" src="/wp-content/uploads/2021/12/F1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">FairScale-A general purpose modular PyTorch library for high performance and large scale training</h3>

		<p class="description">FairScale is a library that extends basic PyTorch capabilities while adding new SOTA techniques for high performance and large scale training on one or multiple machines. FairScale makes available the latest distributed training techniques in the form of composable modules and easy to use APIs. Machine Learning (ML) training at scale traditionally means data parallelism to reduce training time by using multiple devices to train on larger batch size. Nevertheless, with the recent increase of ML models sizes data parallelism is no longer enough to satisfy all &#8220;scaling&#8221; needs. FairScale provides several options to overcome some of the limitations to scale. For scaling training that is bottlenecked by memory (optimizer state, intermediate activations, parameters), FairScale provides APIs that have implemented optimizer, gradient and parameter sharding. This will allow users to train large models using devices in a more memory efficient manner. To overcome the memory required for large models FairScale provides various flavors of pipeline and model parallelism, MOE (Mixture Of Experts) layer, and Offload models. Those methods allow to perform computation only of shards of the models across multiple devices with micro batches of data to maximize device efficiency. FairScale also provides modules to aid users to scale batch size effectively without changing their existing learning rate hyperparameter &#8211; AdaScale &#8211; and save memory with checkpoint activation of intermediate layers. FairScale has also been integrated into Pytorch Lightening, HuggingFace, FairSeq, VISSL, and MMF to enable users of those frameworks to take advantage of its features.</p>

		<div class="card-footer">
							<p class="author">by Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Benjamin Lefaudeux, Vitaliy Liptchinsky, Naman Goyal, Siddhardth Goyal, Myle Ott, Sam Sheifer, Anjali Sridhar, Min Xu</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/F2.png" target="_blank" aria-label="poster Image AdaptDL: An Open-Source Resource-Adaptive Deep Learning Training/Scheduling Framework">
				
<img class="Image image" src="/wp-content/uploads/2021/12/F2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">AdaptDL: An Open-Source Resource-Adaptive Deep Learning Training/Scheduling Framework</h3>

		<p class="description">AdaptDL is an open source framework and scheduling algorithm that directly optimizes cluster-wide training performance and resource utilization. By elastically re-scaling jobs, co-adapting batch sizes and learning rates, and avoiding network interference, AdaptDL improves shared-cluster training compared with alternative schedulers. AdaptDL can automatically determine the optimal number of resources given a job&#8217;s need. It will efficiently add or remove resources dynamically to ensure the highest-level performance. The AdaptDL scheduler will automatically figure out the most efficient number of GPUs to allocate to your job, based on its scalability. When the cluster load is low, your job can dynamically expand to take advantage of more GPUs. AdaptDL offers an easy-to-use API to make existing PyTorch training code elastic with adaptive batch sizes and learning rates. Showcase: Distributed training and Data Loading</p>

		<div class="card-footer">
							<p class="author">by Aurick Qiao, Sang Keun Choe, Suhas Jayaram Subramanya, Willie Neiswanger, Qirong Ho, Hao Zhang, Gregory R. Ganger, Eric P. Xing</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Accelerate PyTorch large model training with ONNX Runtime: just add one line of code!</h3>

		<p class="description">As deep learning models, especially transformer models get bigger and bigger, reducing training time becomes both a financial and environmental imperative. ONNX Runtime can accelerate large-scale distributed training of PyTorch transformer models with a one-line code change (in addition to import statements ;-)) Adding in the DeepSpeed library improves training speed even more. With the new ORTModule API, you wrap an existing torch.nn.Module, and have us automatically: export the model as an ONNX computation graph; compile and optimize it with ONNX Runtime; and integrate it into your existing training script. In this poster, we demonstrate how to fine-tune a popular HuggingFace model and show the performance improvement, on a multi-GPU cluster in the Azure Machine Learning cloud service.<br><br><a href="https://aka.ms/pytorchort" target="_blank">https://aka.ms/pytorchort</a></p>

		<div class="card-footer">
							<p class="author">by Natalie Kershaw</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-distributed-training">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/G2.png" target="_blank" aria-label="poster Image PyTorch/XLA with new Cloud TPU VMs and Profiler">
				
<img class="Image image" src="/wp-content/uploads/2021/12/G2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch/XLA with new Cloud TPU VMs and Profiler</h3>

		<p class="description">PyTorch / XLA enables users to train PyTorch models on XLA devices including Cloud TPUs. Cloud TPU VMs now provide direct access to TPU host machines and hence offer much greater flexibility in addition to making debugging easier and reducing data transfer overheads. PyTorch / XLA has now full support for this new architecture. A new profiling tool has also been developed to enable better profiling of PyTorch / XLA. These improvements not only make it much easier to develop models but also reduce the cost of large-scale PyTorch / XLA training runs on Cloud TPUs.<br><br><a href="http://goo.gle/pt-xla-tpuvm-signup" target="_blank">http://goo.gle/pt-xla-tpuvm-signup</a></p>

		<div class="card-footer">
							<p class="author">by Jack Cao, Daniel Sohn, Zak Stone, Shauheen Zahirazami</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-frontend-experiment-manager">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/E4.png" target="_blank" aria-label="poster Image PyTorch Lightning: Deep Learning without the Boilerplate">
				
<img class="Image image" src="/wp-content/uploads/2021/12/E4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch Lightning: Deep Learning without the Boilerplate</h3>

		<p class="description">PyTorch Lightning reduces the engineering boilerplate and resources required to implement state-of-the-art AI. Organizing PyTorch code with Lightning enables seamless training on multiple-GPUs, TPUs, CPUs, and the use of difficult to implement best practices such as model sharding, 16-bit precision, and more, without any code changes. In this poster, we will use practical Lightning examples to demonstrate how to train Deep Learning models with less boilerplate.<br><br><a href="https://www.pytorchlightning.ai/" target="_blank">https://www.pytorchlightning.ai/</a></p>

		<div class="card-footer">
							<p class="author">by Ari Bornstein</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-frontend-experiment-manager">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/E5.png" target="_blank" aria-label="poster Image Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology">
				
<img class="Image image" src="/wp-content/uploads/2021/12/E5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology</h3>

		<p class="description">Intel and Facebook collaborated to enable BF16, a first-class data type in PyTorch, and a data type that are accelerated natively with the 3rd Gen Intel® Xeon® scalable processors. This poster introduces the latest SW advancements added in Intel Extension for PyTorch (IPEX) on top of PyTorch and the oneAPI DNN library for ease-of-use and high-performance BF16 DL compute on CPU. With these SW advancements, we demonstrated ease-of-use IPEX user-facing API, and we also showcased 1.55X-2.42X speed-up with IPEX BF16 training over FP32 with the stock PyTorch and 1.40X-4.26X speed-up with IPEX BF16 inference over FP32 with the stock PyTorch.<br><br><a href="https://github.com/intel/intel-extension-for-pytorch" target="_blank">https://github.com/intel/intel-extension-for-pytorch</a></p>

		<div class="card-footer">
							<p class="author">by Jiong Gong, Nikita Shustrov, Eikan Wang, Jianhui Li, Vitaly Fedyunin</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-frontend-experiment-manager">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/F4.png" target="_blank" aria-label="poster Image TorchStudio, a machine learning studio software based on PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/F4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">TorchStudio, a machine learning studio software based on PyTorch</h3>

		<p class="description">TorchStudio is a standalone software based on PyTorch and LibTorch. It aims to simplify the creation, training and iterations of PyTorch models. It runs locally on Windows, Ubuntu and macOS. It can load, analyze and explore PyTorch datasets from the TorchVision or TorchAudio categories, or custom datasets with any number of inputs and outputs. PyTorch models can then be loaded and written from scratch, analyzed, and trained using local hardware. Trainings can be run simultaneously and compared to identify the best performing models, and export them as a trained TorchScript or ONNX model.<br><br><a href="https://torchstudio.ai/" target="_blank">https://torchstudio.ai/</a></p>

		<div class="card-footer">
							<p class="author">by Robin Lobel</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-frontend-experiment-manager">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/F5.png" target="_blank" aria-label="poster Image Hydra Framework">
				
<img class="Image image" src="/wp-content/uploads/2021/12/F5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Hydra Framework</h3>

		<p class="description">Hydra is an open source framework for configuring and launching research Python applications. Key features: &#8211; Compose and override your config dynamically to get the perfect config for each run &#8211; Run on remote clusters like SLURM and AWS without code changes &#8211; Perform basic greed search and hyper parameter optimization without code changes &#8211; Command line tab completion for your dynamic config And more.</p>

		<div class="card-footer">
							<p class="author">by Jieru Hu, Omry Yadan</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-frontend-experiment-manager">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/G4.png" target="_blank" aria-label="poster Image PyTorch-Ignite: training common things easy and the hard things possible">
				
<img class="Image image" src="/wp-content/uploads/2021/12/G4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch-Ignite: training common things easy and the hard things possible</h3>

		<p class="description">This poster intends to give a brief but illustrative overview of what PyTorch-Ignite can offer for Deep Learning enthusiasts, professionals and researchers. Following the same philosophy as PyTorch, PyTorch-Ignite aims to keep it simple, flexible and extensible but performant and scalable. Throughout this poster, we will introduce the basic concepts of PyTorch-Ignite, its API and features it offers. We also assume that the reader is familiar with PyTorch.</p>

		<div class="card-footer">
							<p class="author">by Victor Fomin, Sylvain Desroziers, Taras Savchyn</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H4.png" target="_blank" aria-label="poster Image Farabio &#8211; Deep Learning Toolkit for Biomedical Imaging">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Farabio &#8211; Deep Learning Toolkit for Biomedical Imaging</h3>

		<p class="description">Deep learning has transformed many aspects of industrial pipelines recently. Scientists involved in biomedical imaging research are also benefiting from the power of AI to tackle complex challenges. Although the academic community has widely accepted image processing tools, such as scikit-image, ImageJ, there is still a need for a tool which integrates deep learning into biomedical image analysis. We propose a minimal, but convenient Python package based on PyTorch with common deep learning models, extended by flexible trainers and medical datasets.<br><br><a href="https://github.com/tuttelikz/farabio" target="_blank">https://github.com/tuttelikz/farabio</a></p>

		<div class="card-footer">
							<p class="author">by Sanzhar Askaruly, Nurbolat Aimakov, Alisher Iskakov, Hyewon Cho</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H5.png" target="_blank" aria-label="poster Image MONAI: A Domain Specialized Library for Healthcare Imaging">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">MONAI: A Domain Specialized Library for Healthcare Imaging</h3>

		<p class="description">Healthcare image analysis for both radiology and pathology is increasingly being addressed with deep-learning-based solutions. These applications have specific requirements to support various imaging modalities like MR, CT, ultrasound, digital pathology, etc. It is a substantial effort for researchers in the field to develop custom functionalities to handle these requirements. Consequently, there has been duplication of effort, and as a result, researchers have incompatible tools, which makes it hard to collaborate. MONAI stands for Medical Open Network for AI. Its mission is to accelerate the development of healthcare imaging solutions by providing domain-specialized building blocks and a common foundation for the community to converge in a native PyTorch paradigm.<br><br><a href="https://monai.io/" target="_blank">https://monai.io/</a></p>

		<div class="card-footer">
							<p class="author">by Michael Zephyr, Prerna Dogra Richard Brown, Wenqi Li, Eric Kerfoot</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I4.png" target="_blank" aria-label="poster Image How theator Built a Continuous Training Framework to Scale Up Its Surgical Intelligence Platform">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">How theator Built a Continuous Training Framework to Scale Up Its Surgical Intelligence Platform</h3>

		<p class="description">Theator is re-imagining surgery with a Surgical Intelligence platform that leverages highly advanced AI, specifically machine learning and computer vision technology, to analyze every step, event, milestone, and critical junction of surgical procedures. Our platform analyzes lengthy surgical procedure videos and extracts meaningful information, providing surgeons with highlight reels of key moments in an operation, enhanced by annotations. As the team expanded, we realized that we were spending too much time manually running model training and focusing on DevOps tasks and not enough time dedicated to core research. To face this, we build an automation framework composed of multiple training pipelines using PyTorch and ClearML. Our framework automates and manages our entire process, from model development to deployment to continuous training for model improvement. New data is now immediately processed and fed directly into training pipelines – speeding up workflow, minimizing human error, and freeing up our research team for more important tasks. Thus, enabling us to scale our ML operation and deliver better models for our end users.</p>

		<div class="card-footer">
							<p class="author">by Shai Brown, Daniel Neimark, Maya Zohar, Omri Bar, Dotan Asselmann</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I5.png" target="_blank" aria-label="poster Image Q&#038;Aid: A Conversation Agent Powered by PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Q&#038;Aid: A Conversation Agent Powered by PyTorch</h3>

		<p class="description">We present Q&amp;Aid, a conversation agent that relies on a series of machine learning models to filter, label, and answer medical questions based on a provided image and text inputs. Q&amp;Aid is simplifying the hospital logic backend by standardizing it to a Health Intel Provider (HIP). A HIP is a collection of models trained on local data that receives text and visual input, afterward filtering, labeling, and feeding the data to the right models and generating at the end output for the aggregator. Any hospital is identified as a HIP holding custom models and labeling based on its knowledge. The hospitals are training and fine-tuning their models, such as a Visual Question Answering (VQA) model, on private data (e.g. brain anomaly segmentation). We aggregate all of the tasks that the hospitals can provide into a single chat app, offering the results to the user. When the chat ends, the transcript is forwarded to each hospital, a doctor being in charge of the final decision.<br><br><a href="https://qrgo.page.link/d1fQk" target="_blank">https://qrgo.page.link/d1fQk</a></p>

		<div class="card-footer">
							<p class="author">by Cebere Bogdan, Cebere Tudor, Manolache Andrei, Horia Paul-Ion</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J4.png" target="_blank" aria-label="poster Image Sleepbot: Multi-signal Sleep Stage Classifier AI for hospital and home">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Sleepbot: Multi-signal Sleep Stage Classifier AI for hospital and home</h3>

		<p class="description">Sleep disorders and insomnia are now regarded as a worldwide problem. Roughly 62% of adults worldwide feel that they don&#8217;t sleep well. However, sleep is difficult to track so it&#8217;s not easy to get suitable treatment to improve your sleep quality. Currently, the PSG (Polysomnography) is the only way to evaluate the sleep quality accurately but it&#8217;s expensive and often inaccurate due to the first night effect. We propose a multi-signal sleep stage classifier for contactless sleep tracking: Sleepbot. By automating the manual PSG reading and providing explainable analysis, Sleepbot opens a new possibility to apply sleep staging AI in both home and hospital. With sound recorded by a smartphone app and RF-sensed signal measured by Asleep&#8217;s non-contact sleep tracker, Sleepbot provides a clinical level of sleep stage classification. Sleepbot achieved 85.5 % accuracy in 5-class (Wake, N1, N2, N3, Rem) using PSG signals measured from 3,700 subjects and 77 % accuracy in 3-class (Wake, Sleep, REM) classification using only sound data measured from 1,2000 subjects.</p>

		<div class="card-footer">
							<p class="author">by Jaden Hong, Kevin Tran, Tyler Lee, Paul Lee, Freddie Cha, Louis Jung, Dr. Jung Kyung Hong, Dr. In-Young Yoon, David Lee</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyMDE: Minimum-Distortion Embedding</h3>

		<p class="description">We present a unifying framework for the vector embedding problem: given a set of items and some known relationships between them, we seek a representation of the items by vectors, possibly subject to some constraints (e.g., requiring the vectors to have zero mean and identity covariance). We want the vectors associated with similar items to be near each other, and vectors associated with dissimilar items to not be near, measured in Euclidean distance. We formalize this by introducing distortion functions, defined for some pairs of the items. Our goal is to choose an embedding that minimizes the total distortion, subject to the constraints. We call this the minimum-distortion embedding (MDE) problem. The MDE framework generalizes many well-known embedding methods, such as PCA, the Laplacian eigenmap, multidimensional scaling, UMAP, and others, and also includes new types of embeddings. Our accompanying software library, PyMDE, makes it easy for users to specify and approximately solve MDE problems, enabling experimentation with well-known and custom embeddings alike. By making use of automatic differentiation and hardware acceleration via PyTorch, we are able to scale to very large embedding problems. We will showcase examples of embedding real datasets, including an academic co-authorship network, single-cell mRNA transcriptomes, US census data, and population genetics.</p>

		<div class="card-footer">
							<p class="author">by Akshay Agrawal, Alnur Ali, Stephen Boyd</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/K4.png" target="_blank" aria-label="poster Image TorchIO: Pre-Processing &#038; Augmentation of Medical Images for Deep Learning Applications">
				
<img class="Image image" src="/wp-content/uploads/2021/12/K4.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">TorchIO: Pre-Processing &#038; Augmentation of Medical Images for Deep Learning Applications</h3>

		<p class="description">Processing of medical images such as MRI or CT presents unique challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment of volumes. We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be composed, reproduced, traced and extended. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts. TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages open science, as it supports reproducibility and is version controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.</p>

		<div class="card-footer">
							<p class="author">by Fernando Pérez-García, Rachel Sparks, Sébastien Ourselin</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-medical-healthcare">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/K5.png" target="_blank" aria-label="poster Image Deep Learning Based Model to Predict Covid19 Patients&#8217; Outcomes on Admission">
				
<img class="Image image" src="/wp-content/uploads/2021/12/K5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Deep Learning Based Model to Predict Covid19 Patients&#8217; Outcomes on Admission</h3>

		<p class="description">With the extensive use of electronic records and the availability of historical patient information, predictive models that can help identify patients at risk based on their history at an early stage can be a valuable adjunct to clinician judgment. Deep learning models can better predict patients&#8217; outcomes by consuming their medical history regardless of the length and the complexity of such data. We used our Pytorch_EHR framework to train a model that can predict COVID-19 patient&#8217;s health outcomes on admission. We used the Cerner Real-world COVID-19 (Q2) cohort which included information for 117,496 COVID patients from 62 health systems. We used a cohort of 55,068 patients and defined our outcomes including mortality, intubation, and hospitalization longer than 3 days as binary outcomes. We feed the model with all diagnoses, medication, laboratory results, and other clinical events information available before or on their first COVID-19 encounter admission date. We kept the data preprocessing at a minimum for convenience and practicality relying on the embedding layer that learns features representations from the large training set. Our model showed improved performance compared to other baseline machine learning models like logistic regression (LR). For in-hospital mortality, our model showed AUROC of 89.5%, 90.6%, and 84.3% for in-hospital mortality, intubation, and hospitalization for more than 3 days, respectively versus LR which showed 82.8%, 83.2%, and 76.8%<br><br><a href="https://github.com/ZhiGroup/pytorch_ehr" target="_blank">https://github.com/ZhiGroup/pytorch_ehr</a></p>

		<div class="card-footer">
							<p class="author">by Laila Rasmy, Ziqian Xie, Degui Zhi</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A1.png" target="_blank" aria-label="poster Image Rolling out Transformers with TorchScript and Inferentia">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Rolling out Transformers with TorchScript and Inferentia</h3>

		<p class="description">While Transformers have brought unprecedented improvements in the accuracy and ease of developing NLP applications, their deployment remains challenging due to the large size of the models and their computational complexity. Indeed, until recently is has been a widespread misconception that hosting high-performance transformer-based models was prohibitively expensive, and technically challenging. Fortunately, recent advances in both the PyTorch ecosystem and in custom hardware for inference have created a world where models can be deployed in a cost-effective, scalable way, without the need for complex engineering. In this presentation, we will discuss the use of PyTorch and AWS Inferentia to deploy production-scale models in chatbot intent classification &#8211; a particularly relevant and demanding scenario. Autodesk deploys a number of transformer based models to solve customer support issues across our channels, and our ability to provide a flexible, high-quality machine learning solution is supported by leveraging cutting-edge technology such as transformer based classification. Our chatbot, AVA, responds to tens of thousands of customer interactions monthly, and we are evolving our architecture to be supported by customer inference. We will discuss our experience of piloting transformer-based intent models, and present a workflow for going from data to deployment for similar projects.</p>

		<div class="card-footer">
							<p class="author">by Binghui Ouyang, Alexander O’Connor</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A2.png" target="_blank" aria-label="poster Image PyTorchTS: PyTorch Probabilistic Time Series Forecasting Framework">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorchTS: PyTorch Probabilistic Time Series Forecasting Framework</h3>

		<p class="description">PyTorchTS is a PyTorch based Probabilistic Time Series forecasting framework that comes with state of the art univariate and multivariate models.<br><br><a href="https://github.com/zalandoresearch/pytorch-ts" target="_blank">https://github.com/zalandoresearch/pytorch-ts</a></p>

		<div class="card-footer">
							<p class="author">by Kashif Rasul</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A3.png" target="_blank" aria-label="poster Image MMF: A modular framework for multimodal research">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">MMF: A modular framework for multimodal research</h3>

		<p class="description">MMF is designed from ground up to let you focus on what matters &#8212; your model &#8212; by providing boilerplate code for distributed training, common datasets and state-of-the-art pretrained baselines out-of-the-box. MMF is built on top of PyTorch that brings all of its power in your hands. MMF is not strongly opinionated. So you can use all of your PyTorch knowledge here. MMF is created to be easily extensible and composable. Through our modular design, you can use specific components from MMF that you care about. Our configuration system allows MMF to easily adapt to your needs.</p>

		<div class="card-footer">
							<p class="author">by Sasha Sheng, Amanpreet Singh</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">AllenNLP: An NLP research library for developing state-of-the-art models</h3>

		<p class="description">An Apache 2.0 NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.<br><br><a href="https://github.com/allenai/allennlp" target="_blank">https://github.com/allenai/allennlp</a></p>

		<div class="card-footer">
							<p class="author">by Dirk Groeneveld, Akshita Bhagia, Pete Walsh, Michael Schmitz</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/B2.png" target="_blank" aria-label="poster Image Project Spock at Tubi: Understanding Content using Deep Learning for NLP">
				
<img class="Image image" src="/wp-content/uploads/2021/12/B2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Project Spock at Tubi: Understanding Content using Deep Learning for NLP</h3>

		<p class="description">Tubi is one of the leading platforms providing free high-quality streaming movies and TV shows to a worldwide audience. We embrace a data-driven approach and leverage advanced machine learning techniques using PyTorch to enhance our platform and business in any way we can. The Three Pillars of AVOD are the guiding principle for our work. The Pillars are Content: all the titles we maintain in our library Audience: everyone who watches titles on Tubi Advertising: ads shown to viewers on behalf of brands In this poster, we&#8217;ll focus on the Content aspect with more details for the various use cases especially Content Understanding. Content is an important pillar of Tubi since to be successful, we need to look at existing titles and beyond what we already have and attempt to understand all of the titles out in the wild and how they could benefit our platform in some fashion. Content Understanding revolves around digesting a rich collection of 1st- and 3rd-party data in structured (metadata) and unstructured (text) forms and developing representations that capture the essence of those Titles. With the analogy of linear algebra, we can say we are attempting to project Title vectors from the universe to our tubiverse with as much fidelity as possible in order to ascertain potential value for each target use case. We will describe several techniques to understand content better using Pytorch.</p>

		<div class="card-footer">
							<p class="author">by John Trenkle, Jaya Kawale &amp; Tubi ML team</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/B3.png" target="_blank" aria-label="poster Image RL Based Performance Optimization of Deep Neural Networks">
				
<img class="Image image" src="/wp-content/uploads/2021/12/B3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">RL Based Performance Optimization of Deep Neural Networks</h3>

		<p class="description">As the usage of machine learning techniques is becoming ubiquitous, the efficient execution of neural networks is crucial to many applications. Frameworks, such as Halide and TVM, separate the algorithmic representation of the deep learning model from the schedule that determines its implementation. Finding good schedules, however, remains extremely challenging. Auto-tuning methods, which search the space of valid schedules and execute each candidate on the hardware, identify some of the best performing schedules, but the search can take hours, hampering the productivity of deep learning practitioners. What is needed is a method that achieves a similar performance without extensive search, delivering the needed efficiency quickly. Using PyTorch, we model the scheduling process as a sequence of optimization choices, and implement a new technique to accurately predict the expected performance of a partial schedule using a LSTM over carefully engineered features that describe each DNN operator and their current scheduling choices. Leveraging these predictions we are able to make these optimization decisions greedily and, without any executions on the target hardware, rapidly identify an efficient schedule. This techniques enables to find schedules that improve the execution performance of deep neural networks by 2.6× over Halide and 1.5× over TVM. Moreover, our technique completes in seconds instead of hours, making it possible to include it as a new backend for PyTorch itself.<br><br><a href="http://facebook.ai/" target="_blank">http://facebook.ai</a></p>

		<div class="card-footer">
							<p class="author">by Benoit Steiner, Chris Cummins, Horace He, Hugh Leather</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C1.png" target="_blank" aria-label="poster Image A Data-Centric Framework for Composable NLP">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">A Data-Centric Framework for Composable NLP</h3>

		<p class="description">Forte is an open-source toolkit for building Natural Language Processing workflows via assembling state-of-the-art NLP and ML technologies. This toolkit features composable pipeline, cross-task interaction, adaptable data-model interfaces. The highly composable design allows users to build complex NLP pipelines of a wide range of tasks including document retrieval, information extraction, and text generation by combining existing toolkits or customized PyTorch models. The cross-task interaction ability allows developers to utilize the results from individual tasks to make informed decisions. The data-model interface helps developers to focus on building reusable PyTorch models by abstracting out domain and preprocessing details. We show that Forte can be used to build complex pipelines, and the resulting pipeline can be easily adapted to different domains and tasks with small changes in the code.<br><br><a href="https://github.com/asyml/forte" target="_blank">https://github.com/asyml/forte</a></p>

		<div class="card-footer">
							<p class="author">by Zhenghong Liu</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C2.png" target="_blank" aria-label="poster Image Environments and Baselines for Multitask Reinforcement Learning">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Environments and Baselines for Multitask Reinforcement Learning</h3>

		<p class="description">The two key components in a multi-task RL codebase are (i) Multi-task RL algorithms and (ii) Multi-task RL environments. We develop open-source libraries for both components. [MTRL](https://github.com/facebookresearch/mtrl) provides components to implement multi-task RL algorithms, and [MTEnv](https://github.com/facebookresearch/mtenv) is a library to interface with existing multi-task RL environments and create new ones. MTRL has two building blocks: (i) single task policy and (ii) components to augment the single-task policy for multi-task setup. The ideal workflow is to start with a base policy and add multi-task components as they seem fit. MTRL enables algorithms like GradNorm, Distral, HiPBMDP, PCGrad, Soft Modularization, etc. MTEnv is an effort to standardize multi-task RL environments and provide better benchmarks. We extend the Gym API to support multiple tasks, with two guiding principles: (i) Make minimal changes to the Gym Interface (which the community is very familiar with) and (ii) Make it easy to port existing environments to MTEnv. Additionally, we provide a collection of commonly used multi-task RL environments (Acrobot, Cartpole, Multitask variant of DeepMind Control Suite, Meta-World, Multi-armed Bandit, etc.). The RL practitioner can combine its own environments with the MTEnv wrappers to add multi-task support with a small code change. MTRL and MTEnv are used in several ongoing/published works at FAIR.<br><br><a href="http://qr.w69b.com/g/tGZSFw33G" target="_blank">http://qr.w69b.com/g/tGZSFw33G</a></p>

		<div class="card-footer">
							<p class="author">by Shagun Sodhani, Amy Zhang, Ludovic Denoyer, Pierre-Alexandre Kamienny, Olivier Delalleau</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C3.png" target="_blank" aria-label="poster Image The Hugging Face Ecosystem">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">The Hugging Face Ecosystem</h3>

		<p class="description">Transfer learning has become the norm to get state-of-the-art results in NLP. Hugging Face provides you with tools to help you on every step along the way: &#8211; A free git-based shared hub with more than 7,500 PyTorch checkpoints, and more than 800 NLP datasets. &#8211; The ? Datasets library, to easily download the dataset, manipulate it and prepare it. &#8211; The ? Tokenizers library, that provides ultra-fast tokenizers backed by Rust, and converts text in PyTorch tensors. &#8211; The ? Transformers library, providing more than 45 PyTorch implementations of Transformer architectures as simple nn.Module as well as a training API. &#8211; The ? Accelerate library, a non-intrusive API that allows you to run your raw training loop on any distributed setup. The pipeline is then simply a six-step process: select a pretrained model from the hub, handle the data with Datasets, tokenize the text with Tokenizers, load the model with Transformers, train it with the Trainer or your own loop powered by Accelerate, before sharing your results with the community on the hub.<br><br><a href="https://huggingface.co/models" target="_blank">https://huggingface.co/models</a></p>

		<div class="card-footer">
							<p class="author">by Lysandre Debut, Sylvain Gugger, Quentin Lhoest </p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D1.png" target="_blank" aria-label="poster Image Asteroid: the Pytorch-based Audio Source Separation Toolkit for Researchers">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Asteroid: the Pytorch-based Audio Source Separation Toolkit for Researchers</h3>

		<p class="description">Asteroid is an audio source separation toolkit built with PyTorch and PyTorch-Lightning. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, recipes on common audio source separation datasets are provided, including all the steps from data download\preparation through training to evaluation as well as many current state-of-the-art DNN models. Asteroid exposes all levels of granularity to the user from simple layers to complete ready-to-use models. Our pretrained models are hosted on the asteroid-models community in Zenodo and on the Huggingface model Hub. Loading and using pretrained models is trivial and sharing them is also made easy with asteroid&#8217;s CLI.&#8221;,&#8221;poster_showcase&#8221;:&#8221;Audio Source Separation, Speech Processing, Deep Learning&#8221;,&#8221;email&#8221;:&#8221;cornellsamuele@gmail.com&#8221;}<br><br><a href="https://asteroid-team.github.io/" target="_blank">https://asteroid-team.github.io/</a></p>

		<div class="card-footer">
							<p class="author">by Manuel Pariente, Samuele Cornell, Jonas Haag, Joris Cosentino, Michel Olvera, Fabian-Robert Stöter, Efthymios Tzinis</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D2.png" target="_blank" aria-label="poster Image rlstructures: A Lightweight Python Library for Reinforcement Learning Research">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">rlstructures: A Lightweight Python Library for Reinforcement Learning Research</h3>

		<p class="description">RLStructures is a lightweight Python library that provides simple APIs as well as data structures that make as few assumptions as possible about the structure of your agent or your task, while allowing for transparently executing multiple policies on multiple environments in parallel (incl. multiple GPUs). It thus facilitates the implementation of RL algorithms while avoiding complex abstractions.</p>

		<div class="card-footer">
							<p class="author">by Ludovic Denoyer, Danielle Rothermel, Xavier Martinet</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-nlp-multimodal tags-rl-time-series">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D3.png" target="_blank" aria-label="poster Image MBRL-Lib: a PyTorch toolbox for model-based reinforcement learning research">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">MBRL-Lib: a PyTorch toolbox for model-based reinforcement learning research</h3>

		<p class="description">Model-based reinforcement learning (MBRL) is an active area of research with enormous potential. In contrast to model-free RL, MBRL algorithms solve tasks by learning a predictive model of the task dynamics, and use this model to predict the future and facilitate decision making. Many researchers have argued that MBRL can result in lower sample complexity, better generalization, as well as safer and more interpretable decisions. However, despite the surge in popularity and great potential of MBRL, there is currently no widely accepted library for facilitating research in this area. Since MBRL methods often involve the interplay of complex components such as probabilistic ensembles, latent variable models, planning algorithms, and even model-free methods, the lack of such a library raises the entry bar to the field and slows down research efforts. In this work we aim to solve this problem by introducing MBRL-Lib, a modular PyTorch toolbox specifically designed for facilitating research on model-based reinforcement learning. MBRL-Lib provides interchangeable options for training dynamics models and running planning algorithms, which can then be used in a mix and match fashion to create novel MBRL methods. The library also provides a set of utility functions to run common MBRL tasks, as well a set of diagnostics tools to identify potential issues while training dynamics models and control algorithms.<br><br><a href="https://github.com/facebookresearch/mbrl-lib" target="_blank">https://github.com/facebookresearch/mbrl-lib</a></p>

		<div class="card-footer">
							<p class="author">by Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, Roberto Calandra</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-performance-profiler">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H6.png" target="_blank" aria-label="poster Image Introducing New PyTorch Profiler">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Introducing New PyTorch Profiler</h3>

		<p class="description">Analyzing and improving large-scale deep learning model performance is an ongoing challenge that continues to grow in importance as the model sizes increase. Microsoft and Facebook collaborated to create a native PyTorch performance debugging tool called PyTorch Profiler. The profiler builds on the PyTorch autograd profiler foundation, adds a new high fidelity GPU profiling engine, and out-of-the-box bottleneck analysis tool in Tensorboard. New Profiler delivers the simplest experience available to date where users can profile their models without installing any additional packages and see results immediately in Tensorboard. Until today, beginner users of PyTorch may not have attempted to profile their models due to the task complexity. With the new bottleneck analysis tool, they will find profiling easy and accessible. Experienced users will be delighted by the detailed trace views which illustrate GPU kernel execution events and their relationship to the PyTorch operations. Come learn how to profile your PyTorch models using this new delightfully simple tool.<br><br><a href="https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool">https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool</a></p>

		<div class="card-footer">
							<p class="author">by Geeta Chauhan, Gisle Dankel, Elena Neroslavaskaya</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-performance-profiler">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I6.png" target="_blank" aria-label="poster Image TRTorch: A Compiler for TorchScript Targeting NVIDIA GPUs with TensorRT">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">TRTorch: A Compiler for TorchScript Targeting NVIDIA GPUs with TensorRT</h3>

		<p class="description">For experimentation and the development of machine learning models, few tools are as approachable as PyTorch. However, when moving from research to production, some of the features that make PyTorch great for development make it hard to deploy. With the introduction of TorchScript, PyTorch has solid tooling for addressing some of the problems of deploying PyTorch models. TorchScript removes the dependency on Python and produces portable, self contained, static representations of code and weights. But in addition to portability, users also look to optimize performance in deployment. When deploying on NVIDIA GPUs, TensorRT, NVIDIA&#8217;s deep learning optimizer, provides the capability to maximize performance of workloads by tuning the execution of models for specific target hardware. TensorRT also provides tooling for conducting further optimization through mixed and reduced precision execution and post training quantization (PTQ). We present TRTorch, a compiler for PyTorch and TorchScript targeting NVIDIA GPUs, which combines the usability of PyTorch with the performance of TensorRT and allows users to fully optimize their inference workloads without leaving the PyTorch ecosystem. It also simplifies conducting complex optimizations like PTQ by leveraging common PyTorch tooling. TRTorch can be used directly from PyTorch as a TorchScript Backend, embedded in an application or used from the command line to easily increase the performance of inference applications.<br><br><a href="https://nvidia.github.io/TRTorch/" target="_blank">https://nvidia.github.io/TRTorch/</a></p>

		<div class="card-footer">
							<p class="author">by Naren Dasan</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-performance-profiler">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J6.png" target="_blank" aria-label="poster Image WeightWatcher: A Diagnostic Tool for DNNs">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">WeightWatcher: A Diagnostic Tool for DNNs</h3>

		<p class="description">WeightWatcher (WW) is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data. It can be used to: analyze pre/trained pyTorch models; inspect models that are difficult to train; gauge improvements in model performance; predict test accuracies across different models; and detect potential problems when compressing or fine-tuning pretrained models. WeightWatcher is based on theoretical research (done in\-joint with UC Berkeley) into &#8220;Why Deep Learning Works&#8221;, using ideas from Random Matrix Theory (RMT), Statistical Mechanics, and Strongly Correlated Systems.</p>

		<div class="card-footer">
							<p class="author">by Charles H. Martin</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-performance-profiler">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Constrained Optimization in PyTorch 1.9 Through Parametrizations</h3>

		<p class="description">&#8220;This poster presents the &#8220;&#8221;parametrizations&#8221;&#8221; feature that will be added to PyTorch in 1.9.0. This feature allows for a simple implementation of methods like pruning, weight_normalization or spectral_normalization. More generally, it implements a way to have &#8220;&#8221;computed parameters&#8221;&#8221;. This means that we replace a parameter `weight` in a layer with `f(weight)`, where `f` is an arbitrary module. In other words, after putting a parametrization `f` on `layer.weight`, `layer.weight` will return `f(weight)`. They implement a caching system, so that the value `f(weight)` is computed just once during the forward pass. A module that implements a parametrisation may also have a `right_inverse` method. If this method is present, it is possible to assign to a parametrised tensor. This is useful when initialising a parametrised tensor. This feature can be seen as a first step towards invertible modules. In particular, it may also help making distributions first-class citizens in PyTorch. Parametrisations also allows for a simple implementation of constrained optimisation. From this perspective, parametrisation maps an unconstrained tensor to a constrained space such as the space of orthogonal matrices, SPD matrices, low-rank matrices&#8230; This approach is implemented in the library GeoTorch (<a href="https://github.com/Lezcano/geotorch/" target="_blank">https://github.com/Lezcano/geotorch/</a>).&#8221;</p>

		<div class="card-footer">
							<p class="author">by Mario Lezcano-Casado</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H1.png" target="_blank" aria-label="poster Image Distributed Pytorch with Ray">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Distributed Pytorch with Ray</h3>

		<p class="description">Ray is a popular framework for distributed Python that can be paired with PyTorch to rapidly scale machine learning applications. Ray contains a large ecosystem of applications and libraries that leverage and integrate with Pytorch. This includes Ray Tune, a Python library for experiment execution and hyperparameter tuning at any scale; RLlib, a state-of-the-art library for reinforcement learning; and Ray Serve, a library for scalable model serving. Together, Ray and Pytorch are becoming the core foundation for the next generation of production machine learning platforms.<br><br><a href="https://ray.io/" target="_blank">https://ray.io/</a></p>

		<div class="card-footer">
							<p class="author">by Richard Liaw, Kai Fricke, Amog Kamsetty, Michael Galarnyk</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H2.png" target="_blank" aria-label="poster Image Avalanche: an End-to-End Library for Continual Learning based on PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Avalanche: an End-to-End Library for Continual Learning based on PyTorch</h3>

		<p class="description">Learning continually from non-stationary data stream is a long sought goal of machine learning research. Recently, we have witnessed a renewed and fast-growing interest in Continual Learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose an open-source, end-to-end library for continual learning based on PyTorch that may provide a shared and collaborative code-base for fast prototyping, training and reproducible evaluation of continual learning algorithms.<br><br><a href="https://avalanche.continualai.org/" target="_blank">https://avalanche.continualai.org</a></p>

		<div class="card-footer">
							<p class="author">by Vincenzo Lomonaco, Lorenzo Pellegrini Andrea Cossu, Antonio Carta, Gabriele Graffieti</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/H3.png" target="_blank" aria-label="poster Image PyTorch on IBM Z and LinuxONE (s390x)">
				
<img class="Image image" src="/wp-content/uploads/2021/12/H3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch on IBM Z and LinuxONE (s390x)</h3>

		<p class="description">IBM Z is a hardware product line for mission-critical applications, such as finance and health applications. It employs its own CPU architecture, which PyTorch does not officially support. In this poster, we discuss why it is important to support PyTorch on Z. Then, we show our prebuilt minimal PyTorch package for IBM Z. Finally, we demonstrate our continuing commitment to make more PyTorch features available on IBM Z.<br><br><a href="https://codait.github.io/pytorch-on-z" target="_blank">https://codait.github.io/pytorch-on-z</a></p>

		<div class="card-footer">
							<p class="author">by Hong Xu</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I1.png" target="_blank" aria-label="poster Image The Fundamentals of MLOps for R&#038;D: Orchestration, Automation, Reproducibility">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">The Fundamentals of MLOps for R&#038;D: Orchestration, Automation, Reproducibility</h3>

		<p class="description">Both from sanity considerations and the productivity perspective, Data Scientists, ML engineers, Graduate students, and other research-facing roles are all starting to adopt best-practices from production-grade MLOps. However, most toolchains come with a hefty price of extra code and maintenance, which reduces the actual time available for R&amp;D. We will show an alternative approach using ClearML, the open-source MLOps solution. In this &#8220;best-practices&#8221; poster, we will overview the &#8220;must-haves&#8221; of R&amp;D-MLOPs: Orchestration, Automation, and Reproducibility. These enable easy remote execution through magically reproducible setups and even custom, reusable, bottom-up pipelines. We will take a single example and schematically transform it from the &#8220;as downloaded from GitHub&#8221; stage to a fully-fledged, scalable, version-controlled, parameterizable R&amp;D pipeline. We will measure the number of changes needed to the codebase and provide evidence of real low-cost integration. All code, logs, and metrics will be available as supporting information.</p>

		<div class="card-footer">
							<p class="author">by Dr. Ariel Biller</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I2.png" target="_blank" aria-label="poster Image FairTorch: Aspiring to Mitigate the Unfairness of Machine Learning Models">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">FairTorch: Aspiring to Mitigate the Unfairness of Machine Learning Models</h3>

		<p class="description">Is your machine learning model fair enough to be used in your system? What if a recruiting AI discriminates on gender and race? What if the accuracy of medical AI depends on a person&#8217;s annual income or on the GDP of the country where it is used? Today&#8217;s AI has the potential to cause such problems. In recent years, fairness in machine learning has received increasing attention. If current machine learning models used for decision making may cause unfair discrimination, developing a fair machine learning model is an important goal in many areas, such as medicine, employment, and politics. Despite the importance of this goal to society, as of 2020, there was no PyTorch¹ project incorporating fairness into a machine learning model. To solve this problem, we created FairTorch at the PyTorch Summer Hackathon 2020. FairTorch provides a tool to mitigate the unfairness of machine learning models. A unique feature of our tool is that it allows you to add a fairness constraint to your model by adding only a few lines of code, using the fairness criteria provided in the library.<br><br><a href="https://github.com/wbawakate/fairtorch" target="_blank">https://github.com/wbawakate/fairtorch</a></p>

		<div class="card-footer">
							<p class="author">by Masashi Sode, Akihiko Fukuchi, Yoki Yabe, Yasufumi Nakata</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/I3.png" target="_blank" aria-label="poster Image TorchDrift: Drift Detection for PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/I3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">TorchDrift: Drift Detection for PyTorch</h3>

		<p class="description">When machine learning models are deployed to solve a given task, a crucial question is whether they are actually able to perform as expected. TorchDrift addresses one aspect of the answer, namely drift detection, or whether the information flowing through our models &#8211; either probed at the input, output or somewhere in-between &#8211; is still consistent with the one it was trained and evaluated on. In a nutshell, TorchDrift is designed to be plugged into PyTorch models and check whether they are operating within spec. TorchDrift&#8217;s principles apply PyTorch&#8217;s motto _from research to production_ to drift detection: We provide a library of methods that canbe used as baselines or building blocks for drift detection research, as well as provide practitioners deploying PyTorch models in production with up-to-date methods and educational material for building the necessary statistical background. Here we introduce TorchDrift with an example illustrating the underlying two-sample tests. We show how TorchDrift can be integrated in high-performance runtimes such as TorchServe or RedisAI, to enable drift detection in real-world applications thanks to the PyTorch JIT.<br><br><a href="https://torchdrift.org/" target="_blank">https://torchdrift.org/</a></p>

		<div class="card-footer">
							<p class="author">by Thomas Viehmann, Luca Antiga</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J1.png" target="_blank" aria-label="poster Image Ouroboros: MLOps for Automated Driving">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J1.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Ouroboros: MLOps for Automated Driving</h3>

		<p class="description">Modern machine learning for autonomous vehicles requires a fundamentally different infrastructure and production lifecycle from their standard software continuous-integration/continuous-deployment counterparts. At Toyota Research Institute (TRI), we have developed ​Ouroboros​ &#8211; a modern ML platform that supports the end-to-end lifecycle of all ML models delivered to TRI&#8217;s autonomous vehicle fleets. We envision that all ML models delivered to our fleet undergo a systematic and rigorous treatment. Ouroboros delivers several essential features including: a. ML dataset governance and infrastructure-as-code​ that ensures the traceability, reproducibility, standardization, and fairness for all ML datasets and models procedurally generated and delivered to the TRI fleet. b. Unified ML dataset and model management:​ An unified and streamlined workflow for ML dataset curation, label management, and model development that supports several key ML models delivered to the TRI fleet today c. A Large-scale Multi-task, Multi-modal Dataset for Automated Driving​ that supports the development of various models today, including 3D object detection, 2D object detection, 2D BeVFlow, Panoptic Segmentation; d. Orchestrated ML workflows​ to stand up scalable ML applications such as push-button re-training solutions, ML CI/CDs pipelines, Dataset Curation workflows, Auto-labelling pipelines, leveraging the most up-to-date cloud tools available. along their lifecycles, ensuring strong governance on building reusable, reproducible, robust, traceable, and fair ML models for the production driving setting. By following the best MLOps practices, we expect our platform to lay the foundation for continuous life-long learning in our autonomous vehicle fleets and accelerate the transition from research to production.<br><br><a href="https://github.com/TRI-ML" target="_blank">https://github.com/TRI-ML</a></p>

		<div class="card-footer">
							<p class="author">by Quincy Chen, Arjun Bhargava, Sudeep Pillai, Marcus Pan, Chao Fang, Chris Ochoa, Adrien Gaidon, Kuan-Hui Lee, Wolfram Burgard</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J2.png" target="_blank" aria-label="poster Image carefree-learn: Tabular Datasets ❤️ PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">carefree-learn: Tabular Datasets ❤️ PyTorch</h3>

		<p class="description">carefree-learn makes PyTorch accessible to people who are familiar with machine learning but not necessarily PyTorch. By having already implemented all the pre-processing and post-processing under the hood, users can focus on implementing the core machine learning algorithms / models with PyTorch and test them on various datasets. By having designed the whole structure carefully, users can easily customize every block in the whole pipeline, and can also &#8216;combine&#8217; the implemented blocks to &#8216;construct&#8217; new models without efforts. By having carefully made abstractions users can adapt it to their specific down-stream tasks, such as quantitative trading (in fact I&#8217;ve already implemented one for my company and it works pretty well XD). carefree-learn handles distributed training carefully, so users can either run multiple tasks at the same time, or run a huge model with DDP in one line of code. carefree-learn also integrates with mlflow and supports exporting to ONNX, which means it is ready for production to some extend.</p>

		<div class="card-footer">
							<p class="author">by Yujian He</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/J3.png" target="_blank" aria-label="poster Image OpenMMLab: An Open-Source Algorithm Platform for Computer Vision">
				
<img class="Image image" src="/wp-content/uploads/2021/12/J3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">OpenMMLab: An Open-Source Algorithm Platform for Computer Vision</h3>

		<p class="description">OpenMMLab project builds open-source toolboxes for Artificial Intelligence (AI). It aims to 1) provide high-quality codebases to reduce the difficulties in algorithm reimplementation; 2) provide a complete research platform to accelerate the research production; and 3) shorten the gap between research production to the industrial applications. Based on PyTorch, OpenMMLab develops MMCV to provide unified abstract training APIs and common utils, which serves as a foundation of 15+ toolboxes and 40+ datasets. Since the initial release in October 2018, OpenMMLab has released 15+ toolboxes that cover 10+ directions, implement 100+ algorithms, and contain 1000+ pre-trained models. With a tighter collaboration with the community, OpenMMLab will release more toolboxes with more flexible and easy-to-use training frameworks in the future.<br><br><a href="https://openmmlab.com/" target="_blank">https://openmmlab.com/</a></p>

		<div class="card-footer">
							<p class="author">by Wenwei Zhang</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/K2.png" target="_blank" aria-label="poster Image Catalyst – Accelerated deep learning R&#038;D">
				
<img class="Image image" src="/wp-content/uploads/2021/12/K2.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Catalyst – Accelerated deep learning R&#038;D</h3>

		<p class="description">For the last three years, Catalyst-Team and collaborators have been working on Catalyst  &#8211; a high-level PyTorch framework Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. You get metrics, model checkpointing, advanced logging, and distributed training support without the boilerplate and low-level bugs.<br><br><a href="https://catalyst-team.com/" target="_blank">https://catalyst-team.com</a></p>

		<div class="card-footer">
							<p class="author">by Sergey Kolesnikov</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-platforms-ops-tools">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/K3.png" target="_blank" aria-label="poster Image High-fidelity performance metrics for generative models in PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/K3.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">High-fidelity performance metrics for generative models in PyTorch</h3>

		<p class="description">Evaluation of generative models such as GANs is an important part of deep learning research. In 2D image generation, three approaches became widely spread: Inception Score, Fréchet Inception Distance, and Kernel Inception Distance. Despite having a clear mathematical and algorithmic description, these metrics were initially implemented in TensorFlow and inherited a few properties of the framework itself, such as a specific implementation of the interpolation function. These design decisions were effectively baked into the evaluation protocol and became an inherent part of the specification of the metrics. As a result, researchers wishing to compare against state of the art in generative modeling are forced to perform an evaluation using the original metric authors&#8217; codebases. Reimplementations of metrics in PyTorch and other frameworks exist, but they do not provide a proper level of fidelity, thus making them unsuitable for reporting results and comparing them to other methods. This software aims to provide epsilon-exact implementations of the said metrics in PyTorch and remove inconveniences associated with generative model evaluation and development. All the evaluation pipeline steps are correctly tested, with relative errors and sources of remaining non-determinism summarized in sections below. TLDR; fast and reliable GAN evaluation in PyTorch<br><br><a href="https://github.com/toshas/torch-fidelity" target="_blank">https://github.com/toshas/torch-fidelity</a></p>

		<div class="card-footer">
							<p class="author">by Anton Obukhov</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A6.png" target="_blank" aria-label="poster Image Using Satellite Imagery to Identify Oceanic Oil Pollution">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Using Satellite Imagery to Identify Oceanic Oil Pollution</h3>

		<p class="description">Operational oil discharges from ships, also known as &#8220;bilge dumping,&#8221; have been identified as a major source of petroleum products entering our oceans, cumulatively exceeding the largest oil spills, such as the Exxon Valdez and Deepwater Horizon spills, even when considered over short time spans. However, we still don&#8217;t have a good estimate of ● How much oil is being discharged; ● Where the discharge is happening; ● Who the responsible vessels are. This makes it difficult to prevent and effectively respond to oil pollution that can damage our marine and coastal environments and economies that depend on them. In this poster we will share SkyTruth&#8217;s recent work to address these gaps using machine learning tools to detect oil pollution events and identify the responsible vessels when possible. We use a convolutional neural network (CNN) in a ResNet-34 architecture to perform pixel segmentation on all incoming Sentinel-1 synthetic aperture radar (SAR) imagery to classify slicks. Despite the satellites&#8217; incomplete oceanic coverage, we have been detecting an average of 135 vessel slicks per month, and have identified several geographic hotspots where oily discharges are occurring regularly. For the images that capture a vessel in the act of discharging oil, we rely on an Automatic Identification System (AIS) database to extract details about the ships, including vessel type and flag state. We will share our experience ● Making sufficient training data from inherently sparse satellite image datasets; ● Building a computer vision model using PyTorch and fastai; ● Fully automating the process in the Amazon Web Services (AWS) cloud. The application has been running continuously since August 2020, has processed over 380,000 Sentinel-1 images, and has populated a database with more than 1100 high-confidence slicks from vessels. We will be discussing preliminary results from this dataset and remaining challenges to be overcome. Learn more at <a href="https://skytruth.org/bilge-dumping/" target="_blank">https://skytruth.org/bilge-dumping/</a></p>

		<div class="card-footer">
							<p class="author">by Jona Raphael (jona@skytruth.org), Ben Eggleston, Ryan Covington, Tatianna Evanisko, John Amos</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A7.png" target="_blank" aria-label="poster Image UPIT: A fastai Package for Unpaired Image-to-Image Translation">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A7.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">UPIT: A fastai Package for Unpaired Image-to-Image Translation</h3>

		<p class="description">Unpaired image-to-image translation algorithms have been used for various computer vision tasks like style transfer and domain adaption. Such algorithms are highly attractive because they alleviate the need for the collection of paired datasets. In this poster, we demonstrate UPIT, a novel fastai/PyTorch package (built with nbdev) for unpaired image-to-image translation. It implements various state-of-the-art unpaired image-to-image translation algorithms such as CycleGAN, DualGAN, UNIT, and more. It enables simple training and inference on unpaired datasets. It also comes with implementations of commonly used metrics like FID, KID, and LPIPS. It also comes with Weights-and-Biases integration for easy experiment tracking. Since it is built on top of fastai and PyTorch, it comes with support for mixed-precision and multi-GPU training. It is highly flexible, and custom dataset types, models, and metrics can be used as well. With UPIT, training and applying unpaired image-to-image translation only takes a few lines of code.<br><br><a href="https://github.com/tmabraham/UPIT" target="_blank">https://github.com/tmabraham/UPIT</a></p>

		<div class="card-footer">
							<p class="author">by Tanishq Abraham</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A8.png" target="_blank" aria-label="poster Image PyTorchVideo: A Deep Learning Library for Video Understanding">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorchVideo: A Deep Learning Library for Video Understanding</h3>

		<p class="description">PyTorchVideo is the new Facebook AI deep learning library for video understanding research. It contains variety of state of the art pretrained video models, dataset, augmentation, tools for video understanding. PyTorchVideo provides efficient video components on accelerated inference on mobile device.<br><br><a href="https://pytorchvideo.org/" target="_blank">https://pytorchvideo.org/</a></p>

		<div class="card-footer">
							<p class="author">by Aaron Adcock, Bo Xiong, Christoph Feichtenhofer, Haoqi Fan, Heng Wang, Kalyan Vasudev Alwala, Matt Feiszli, Tullie Murrell, Wan-Yen Lo, Yanghao Li, Yilei Li, Zhicheng Yan</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Deep Learning Enables Fast and Dense Single-Molecule Localization with High Accuracy</h3>

		<p class="description">Single-molecule localization microscopy (SMLM) has had remarkable success in imaging cellular structures with nanometer resolution, but the need for activating only single isolated emitters limits imaging speed and labeling density. Here, we overcome this major limitation using deep learning. We developed DECODE, a computational tool that can localize single emitters at high density in 3D with the highest accuracy for a large range of imaging modalities and conditions. In a public software benchmark competition, it outperformed all other fitters on 12 out of 12 data-sets when comparing both detection accuracy and localization error, often by a substantial margin. DECODE allowed us to take live-cell SMLM data with reduced light exposure in just 3 seconds and to image microtubules at ultra-high labeling density. Packaged for simple installation and use, DECODE will enable many labs to reduce imaging times and increase localization density in SMLM.<br><br><a href="http://github.com/turagalab/decode" target="_blank">http://github.com/turagalab/decode</a></p>

		<div class="card-footer">
							<p class="author">by A. Speiser, L-R. Müller, P. Hoess, U. Matti, C. J. Obara, J. H. Macke, J. Ries, S. C. Turaga</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/B7.png" target="_blank" aria-label="poster Image A Robust PyTorch Trainable Entry Convnet Layer in Fourier Domain">
				
<img class="Image image" src="/wp-content/uploads/2021/12/B7.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">A Robust PyTorch Trainable Entry Convnet Layer in Fourier Domain</h3>

		<p class="description">We draw inspiration from the cortical area V1. We try to mimic their main processing properties by means of: quaternion local phase/orientation to compute lines and edges detection in a specific direction. We analyze how this layer is robust by its greometry to large illumination and brightness changes.<br><br><a href="https://gitlab.com/ab.sanchezperez/pytorch-monogenic" target="_blank">https://gitlab.com/ab.sanchezperez/pytorch-monogenic</a></p>

		<div class="card-footer">
							<p class="author">by Abraham Sánchez, Guillermo Mendoza, E. Ulises Moya-Sánchez</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/B8.png" target="_blank" aria-label="poster Image PyroNear: Embedded Deep Learning for Early Wildfire Detection">
				
<img class="Image image" src="/wp-content/uploads/2021/12/B8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyroNear: Embedded Deep Learning for Early Wildfire Detection</h3>

		<p class="description">&#8220;PyroNear is non-profit organization composed solely of volunteers which was created in late 2019. Our core belief is that recent technological developments can support the cohabitation between mankind &amp; its natural habitat. We strive towards high-performing, accessible &amp; affordable tech-solutions for protection against natural hazards. More specifically, our first efforts are focused on wildfire protection by increasing the coverage of automatic detection systems. Our ongoing initiative has now gathered dozens of volunteers to put up the following main contributions: &#8211; Computer Vision: compiling open-source models and datasets (soon to be published) for vision tasks related to wildfire detection &#8211; Edge Computing: developing an affordable physical prototype running our PyTorch model on a Raspberry Pi &#8211; End-to-end detection workflow: building a responsible end-to-end system for large scale detection and alert management (API, front-end monitoring platform) &#8211; Deployment: working with French firefighter departments to gather field knowledge and conduct a test phase over the incoming European summer.&#8221; PyTorch3D is a modular and optimized library for 3D Deep Learning with PyTorch. It includes support for: data structures for heterogeneous batching of 3D data (Meshes, Point clouds and Volumes), optimized 3D operators and loss functions (with custom CUDA kernels), a modular differentiable rendering API for Meshes, Point clouds and Implicit functions, as well as several other tools for 3D Deep Learning.<br><br><a href="https://github.com/pyronear" target="_blank">https://github.com/pyronear</a></p>

		<div class="card-footer">
							<p class="author">by François-Guillaume Fernandez, Mateo Lostanlen, Sebastien Elmaleh, Bruno Lenzi, Felix Veith, and more than 15+ contributors</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C6.png" target="_blank" aria-label="poster Image PyTorch3D: Fast, Flexible, 3D Deep Learning">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">PyTorch3D: Fast, Flexible, 3D Deep Learning</h3>

		<p class="description">PyTorch3D is a modular and optimized library for 3D Deep Learning with PyTorch. It includes support for: data structures for heterogeneous batching of 3D data (Meshes, Point clouds and Volumes), optimized 3D operators and loss functions (with custom CUDA kernels), a modular differentiable rendering API for Meshes, Point clouds and Implicit functions, as well as several other tools for 3D Deep Learning.<br><br><a href="https://arxiv.org/abs/2007.08501" target="_blank">https://arxiv.org/abs/2007.08501</a></p>

		<div class="card-footer">
							<p class="author">by Nikhila Ravi, Jeremy Reizenstein, David Novotny, Justin Johnson, Georgia Gkioxari, Roman Shapovalov, Patrick Labatut, Wan-Yen Lo</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C7.png" target="_blank" aria-label="poster Image Kornia: an Open Source Differentiable Computer Vision Library for PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C7.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Kornia: an Open Source Differentiable Computer Vision Library for PyTorch</h3>

		<p class="description">This work presents Kornia, an open source computer vision library built upon a set of differentiable routines and modules that aims to solve generic computer vision problems. The package uses PyTorch as its main backend, not only for efficiency but also to take advantage of the reverse auto-differentiation engine to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be integrated into neural networks to train models to perform a wide range of operations including image transformations,camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations on graphical processing units, generating faster systems. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.<br><br><a href="http://www.kornia.org/" target="_blank">http://www.kornia.org</a></p>

		<div class="card-footer">
							<p class="author">by E. Riba, J. Shi, D. Mishkin, L. Ferraz, A. Nicolao</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/C8.png" target="_blank" aria-label="poster Image NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in PyTorch">
				
<img class="Image image" src="/wp-content/uploads/2021/12/C8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">NNGeometry: Easy and Fast Fisher Information Matrices and Neural Tangent Kernels in PyTorch</h3>

		<p class="description">Fisher Information Matrices (FIM) and Neural Tangent Kernels (NTK) are useful tools in a number of diverse applications related to neural networks. Yet these theoretical tools are often difficult to implement using current libraries for practical size networks, given that they require per-example gradients, and a large amount of memory since they scale as the number of parameters (for the FIM) or the number of examples x cardinality of the output space (for the NTK). NNGeometry is a PyTorch library that offers a high level API for computing various linear algebra operations such as matrix-vector products, trace, frobenius norm, and so on, where the matrix is either the FIM or the NTK, leveraging recent advances in approximating these matrices.<br><br><a href="https://github.com/tfjgeorge/nngeometry/" target="_blank">https://github.com/tfjgeorge/nngeometry/</a></p>

		<div class="card-footer">
							<p class="author">by Thomas George</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D6.png" target="_blank" aria-label="poster Image CompressAI: a research library and evaluation platform for end-to-end compression">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D6.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">CompressAI: a research library and evaluation platform for end-to-end compression</h3>

		<p class="description">CompressAI is a PyTorch library that provides custom operations, layers, modules and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, CompressAI includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. State-of-the-art end-to-end compression models have been reimplemented in PyTorch and trained from scratch, reproducing published results and allowing further research in the domain.</p>

		<div class="card-footer">
							<p class="author">by Bégaint J., Racapé F., Feltman S., Pushparaja A.</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D7.png" target="_blank" aria-label="poster Image pystiche: A Framework for Neural Style Transfer">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D7.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">pystiche: A Framework for Neural Style Transfer</h3>

		<p class="description">The seminal work of Gatys, Ecker, and Bethge gave birth to the field of <em>Neural Style Transfer</em> (NST) in 2016. An NST describes the merger between the content and artistic style of two arbitrary images. This idea is nothing new in the field of Non-photorealistic rendering (NPR). What distinguishes NST from traditional NPR approaches is its generality: an NST only needs a single arbitrary content and style image as input and thus &#8220;makes &#8212; for the first time &#8212; a generalized style transfer practicable&#8221;. Besides peripheral tasks, an NST at its core is the definition of an optimization criterion called <em>perceptual loss</em>, which estimates the perceptual quality of the stylized image. Usually the perceptual loss comprises a deep neural network that needs to supply encodings of images from various depths. <code>pystiche</code> is a library for NST written in Python and built upon PyTorch. It provides modular and efficient implementations for commonly used perceptual losses as well as neural net architectures. This enables users to mix current state-of-the-art techniques with new ideas with ease. This poster will showcase the core concepts of <code>pystiche</code> that will enable other researchers as well as lay persons to got an NST running in minutes.<br><br><a href="https://github.com/pmeier/pystiche" target="_blank">https://github.com/pmeier/pystiche</a></p>

		<div class="card-footer">
							<p class="author">by Philip Meier, Volker Lohweg</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-vision">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/D8.png" target="_blank" aria-label="poster Image GaNDLF – A Generally Nuanced Deep Learning Framework for Clinical Imaging Workflows">
				
<img class="Image image" src="/wp-content/uploads/2021/12/D8.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">GaNDLF – A Generally Nuanced Deep Learning Framework for Clinical Imaging Workflows</h3>

		<p class="description">Deep Learning (DL) has greatly highlighted the potential impact of optimized machine learning in both the scientific and clinical communities. The advent of open-source DL libraries from major industrial entities, such as TensorFlow (Google), PyTorch (Facebook), further contributes to DL promises on the democratization of computational analytics. However, increased technical and specialized background is required to develop DL algorithms, and the variability of implementation details hinders their reproducibility. Towards lowering the barrier and making the mechanism of DL development, training, and inference more stable, reproducible, and scalable, without requiring an extensive technical background, this manuscript proposes the Generally Nuanced Deep Learning Framework (GaNDLF). With built-in support for k-fold cross-validation, data augmentation, multiple modalities and output classes, and multi-GPU training, as well as the ability to work with both radiographic and histologic imaging, GaNDLF aims to provide an end-to-end solution for all DL-related tasks, to tackle problems in medical imaging and provide a robust application framework for deployment in clinical workflows. Keywords: Deep Learning, Framework, Segmentation, Regression, Classification, Cross-validation, Data augmentation, Deployment, Clinical, Workflows</p>

		<div class="card-footer">
							<p class="author">by Siddhish Thakur</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Quantization-Aware Training with Brevitas</h3>

		<p class="description">Brevitas is an open-source PyTorch library for quantization-aware training. Thanks to its flexible design at multiple levels of abstraction, Brevitas generalizes the typical uniform affine quantization paradigm adopted in the deep learning community under a common set of unified APIs. Brevitas provides a platform to both ML practitioners and researchers to either apply built-in state-of-the-art techniques in training for reduced-precision inference, or to implement novel quantization-aware training algorithms. Users can target supported inference toolchains, such as onnxruntime, TVM, Vitis AI, FINN or PyTorch itself, or experiment with hypothetical target hardware platforms. In particular, when combined with the flexibility of Xilinx FPGAs through the FINN toolchain, Brevitas supports the co-design of novel hardware building blocks in a machine-learning driven fashion. Within Xilinx, Brevitas has been adopted by various research projects concerning quantized neural networks, as well as in large scale deployments targeting custom programmable logic accelerators.<br><br><a href="https://github.com/Xilinx/brevitas/" target="_blank">https://github.com/Xilinx/brevitas/</a></p>

		<div class="card-footer">
							<p class="author">by Alessandro Pappalardo</p>
			
					</div>
	</div>
</div>
</article>
									
<article class="FeedCard hide " data-filter-ids="tags-compiler-transform-production">
	  
<div class="PosterCard ">
	<div class="thumbnail">
					<a href="/wp-content/uploads/2021/12/A5.png" target="_blank" aria-label="poster Image Upcoming features in TorchScript">
				
<img class="Image image" src="/wp-content/uploads/2021/12/A5.png" alt="">
			</a>
			</div>

	<div class="content poster-content">
		<h4 class="posttype">POSTER</h4>

		<h3 class="title">Upcoming features in TorchScript</h3>

		<p class="description">TorchScript is the bridge between PyTorch&#8217;s flexible eager mode to more deterministic and performant graph mode suitable for production deployment. As part of PyTorch 1.9 release, TorchScript will launch a few features that we&#8217;d like to share with you earlier, including a) a new formal language specification that defines the exact subset of Python/PyTorch features supported in TorchScript; b) Profile-Directed Typing that reduces the burden of converting a loosely-typed eager model into a strictly-typed TorchScript model; c) A TorchScript profiler that can shed light on performance characteristics of TorchScript model. We are constantly making improvements to make TorchScript easier to use and more performant.<br><br><a href="http://fb.me/torchscript" target="_blank">http://fb.me/torchscript</a></p>

		<div class="card-footer">
							<p class="author">by Yanan Cao, Harry Kim, Jason Ansel</p>
			
					</div>
	</div>
</div>
</article>
							</div>
			
<button class="SeeMore see-more" data-load-per-click="5" data-initial-load="5" data-container-class="block-21930c15-7778-4fa0-9c3c-b80eeaa7e832" data-item-class="FeedCard">See More</button>
		</section>
	</div>
</div>
	</main>

	<footer class="Footer">
		<div class="mainNav">
			<div class="logo">
				<a href="/" aria-label="Go to Pytorch home">
					<svg width="30" height="35" viewbox="0 0 30 35" fill="none" xmlns="http://www.w3.org/2000/svg" role="img">
<path d="M24.8384 10.3748L22.3344 12.8296C26.5077 16.9211 26.5077 23.5842 22.3344 27.6757C18.1612 31.7671 11.3647 31.7671 7.19148 27.6757C3.01823 23.5842 3.01823 16.9211 7.19148 12.8296L13.8687 6.28337L14.7033 5.34818V0.438477L4.56829 10.3748C-1.03579 15.869 -1.03579 24.6363 4.56829 30.1305C10.1724 35.6247 19.1151 35.6247 24.7191 30.1305C30.4424 24.6363 30.4424 15.7521 24.8384 10.3748Z" fill="#F05F42"></path>
<ellipse cx="19.8316" cy="7.80298" rx="1.90777" ry="1.87036" fill="#F05F42"></ellipse>
</svg>
				</a>
			</div>

			<ul class="footerNav">
													<li class="mainItem">
						<a href="#" target="" class="footer-item  ">
							PyTorch						</a>
													<ul>
																	<li class="subItem">
										<a href="/install/" target="" class=" ">
											Install										</a>
																			</li>
																	<li class="subItem">
										<a href="/features/" target="" class=" ">
											Features										</a>
																			</li>
																	<li class="subItem">
										<a href="/resources/" target="" class=" ">
											Resources										</a>
																			</li>
																	<li class="subItem">
										<a href="https://pytorch.org/docs/stable/index.html" target="" class="docs ">
											Docs										</a>
																			</li>
																	<li class="subItem">
										<a href="https://pytorch.org/tutorials/" target="" class="tutorials ">
											Tutorials										</a>
																			</li>
																	<li class="subItem">
										<a href="/community/" target="" class=" ">
											Community										</a>
																			</li>
																	<li class="subItem">
										<a href="https://github.com/pytorch/pytorch" target="" class=" ">
											Github										</a>
																			</li>
															</ul>
											</li>
									<li class="mainItem">
						<a href="#" target="" class="footer-item  ">
							Support						</a>
													<ul>
																	<li class="subItem">
										<a href="https://github.com/pytorch/pytorch/issues" target="_blank" class=" icon-github">
											Github Issues										</a>
																			</li>
																	<li class="subItem">
										<a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank" class=" ">
											Brand Guidelines										</a>
																			</li>
																	<li class="subItem">
										<a href="https://discuss.pytorch.org/" target="_blank" class=" ">
											Discuss										</a>
																			</li>
															</ul>
											</li>
								<li class="mainItem">
					
<div class="navItems-followContainer followMenu ">
			<div class="mainItem">Follow Us</div>
		<ul class="follow-list">
					<li class="followItem">
				<a aria-label="follow Menu  icon-twitter" href="https://twitter.com/pytorch" target="_blank" class=" icon-twitter">
					Twitter				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-facebook" href="https://www.facebook.com/pytorch" target="_blank" class=" icon-facebook">
					Facebook				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-youtube" href="https://www.youtube.com/pytorch" target="_blank" class=" icon-youtube">
					YouTube				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-linkedin" href="https://www.linkedin.com/company/pytorch" target="_blank" class=" icon-linkedin">
					Linkedin				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-medium" href="https://medium.com/pytorch" target="_blank" class=" icon-medium">
					Medium				</a>
			</li>
					<li class="followItem">
				<a aria-label="follow Menu  icon-github" href="https://github.com/pytorch/pytorch" target="_blank" class=" icon-github">
					GitHub				</a>
			</li>
			</ul>
</div>
				</li>
			</ul>
		</div>
		<div class="legalNav">
			<ul class="legalNavList">
													<li>
						<a href="/wp-content/uploads/2021/11/fb-tos-privacy-policy.pdf" target="" class=" ">
							Terms						</a>
					</li>
									<li>
						<a href="/wp-content/uploads/2021/11/fb-oss-privacy-policy.pdf" target="" class=" ">
							Privacy						</a>
					</li>
							</ul>
		</div>
	</footer>
</div>

<script id="pytorch-classic-js-js-extra">
var gridData = {"build":{"stable":"Stable (1.9.0)","nightly":"Preview (Nightly)","lts":"LTS (1.8.1)"},"os":{"mac":"MacOS","linux":"Linux","windows":"Windows"},"package":{"conda":"Conda","pip":"Pip","libtorch":"LibTorch","source":"Source"},"language":{"python":"Python","cplusplus":"C++\/Java"},"platform":{"cuda10.2":"CUDA 10.2","cuda11.1":"CUDA 11.1","rocm":"ROCm 4.2(beta)","cpu":"CPU"}};
var platformOSSupport = {"linux":["cuda10.2","rocm","cpu"],"mac":["cpu"],"windows":["cuda10.2","rocm","cpu"]};
var packageLanguageSupport = {"python":["pip","conda","source"],"cplusplus":["libtorch"]};
var commands = {"stable,conda,mac,cpu,python":"conda install pytorch torchvision torchaudio -c pytorch","nightly,conda,linux,cpu,python":"conda install pytorch torchvision torchaudio cpuonly -c pytorch-nightly","stable,libtorch,windows,rocm,cplusplus":"NOTE: ROCm is not available on Windows","stable,conda,windows,cpu,python":"conda install pytorch torchvision torchaudio cpuonly -c pytorch-nightly","stable,conda,windows,rocm,python":"NOTE: ROCm is not available on Windows","stable,pip,windows,rocm,python":"NOTE: ROCm is not available on Windows","stable,pip,windows,cpu,python":"pip3 install --pre torch torchvision torchaudio -f https:\/\/download.pytorch.org\/whl\/nightly\/cpu\/torch_nightly.html","stable,conda,windows,cuda10.2,python":"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-nightly","stable,conda,windows,cuda11.1,python":"NOTE: \\'conda-forge\\' channel is required for cudatoolkit 11.1\r\nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-nightly -c conda-forge","stable,libtorch,mac,rocm,cplusplus":"NOTE: ROCm is not available on MacOS","stable,conda,mac,rocm,python":"NOTE: ROCm is not available on MacOS","stable,conda,linux,cpu,python":"conda install pytorch torchvision torchaudio cpuonly -c pytorch","stable,conda,linux,rocm,python":"NOTE: Conda packages are not currently available for ROCm, please use pip instead","stable,conda,linux,cuda10.2,python":"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch","stable,conda,linux,cuda11.1,python":"NOTE: \\'nvidia\\' channel is required for cudatoolkit 11.1\r\nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia","lts,libtorch,windows,rocm,cplusplus":"NOTE: ROCm is not supported in LTS","lts,pip,mac,cpu,python":"# macOS is not currently supported for lts","lts,conda,mac,cuda10.2,python":"# macOS is not currently supported for lts","lts,conda,mac,cuda11.1,python":"# macOS is not currently supported for lts","lts,conda,mac,rocm,python":"# macOS is not currently supported for lts","lts,conda,mac,cpu,python":"# macOS is not currently supported for lts","stable,conda,mac,cuda10.2,python":"# MacOS Binaries dont support CUDA, install from source if CUDA is needed\r\nconda install pytorch torchvision torchaudio -c pytorch","lts,libtorch,mac,cpu,cplusplus":"# macOS is not currently supported for lts","lts,libtorch,mac,cuda10.2,cplusplus":"# macOS is not currently supported for lts","lts,libtorch,mac,cuda11.1,cplusplus":"# macOS is not currently supported for lts","lts,libtorch,mac,rocm,cplusplus":"# macOS is not currently supported for lts","lts,pip,windows,rocm,python":"NOTE: ROCm is not supported in LTS","stable,libtorch,linux,cpu,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-shared-with-deps-1.9.1%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-shared-with-deps-1.9.1%2Bcpu.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcpu.zip<\/a>","stable,pip,linux,cpu,python":"pip3 install --pre torch torchvision torchaudio -f https:\/\/download.pytorch.org\/whl\/nightly\/cpu\/torch_nightly.html","stable,pip,linux,cuda11.1,python":"pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","stable,pip,linux,cuda10.2,python":"pip3 install torch torchvision torchaudio","stable,pip,linux,rocm,python":"pip3 install torch torchvision==0.10.1 -f https:\/\/download.pytorch.org\/whl\/rocm4.2\/torch_stable.html","stable,libtorch,linux,cuda10.2,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu102\/libtorch-shared-with-deps-1.9.1%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/cu102\/libtorch-shared-with-deps-1.9.1%2Bcu102.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu102\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/cu102\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu102.zip<\/a>","stable,libtorch,linux,cuda11.1,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-shared-with-deps-1.9.1%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-shared-with-deps-1.9.1%2Bcu111.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu111.zip<\/a>","stable,libtorch,linux,rocm,cplusplus":"LibTorch binaries are not available for ROCm, please build it from source","stable,pip,mac,cuda10.2,python":"# MacOS Binaries dont support CUDA, install from source if CUDA is needed\r\npip3 install torch torchvision torchaudio","stable,pip,mac,cuda11.1,python":"# MacOS Binaries dont support CUDA, install from source if CUDA is needed\r\npip3 install torch torchvision torchaudio","stable,pip,mac,rocm,python":"NOTE: ROCm is not available on MacOS","stable,pip,mac,cpu,python":"pip3 install torch torchvision torchaudio","stable,conda,mac,cuda11.1,python":"# MacOS Binaries dont support CUDA, install from source if CUDA is needed\r\nconda install pytorch torchvision torchaudio -c pytorch","stable,libtorch,mac,cpu,cplusplus":"Download here:\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip\">https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip<\/a>","stable,libtorch,mac,cuda10.2,cplusplus":"MacOS binaries do not support CUDA. Download CPU libtorch here:\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip\">https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip<\/a>","stable,libtorch,mac,cuda11.1,cplusplus":"MacOS binaries do not support CUDA. Download CPU libtorch here:\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip\">https:\/\/download.pytorch.org\/libtorch\/cpu\/libtorch-macos-1.9.1.zip<\/a>","stable,libtorch,windows,cuda11.1,python":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-win-shared-with-deps-1.9.1%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-win-shared-with-deps-1.9.1%2Bcu111.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-win-shared-with-deps-debug-1.9.1%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/cu111\/libtorch-win-shared-with-deps-debug-1.9.1%2Bcu111.zip<\/a>","stable,libtorch,windows,cuda10.2,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cu102\/libtorch-win-shared-with-deps-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cu102\/libtorch-win-shared-with-deps-latest.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cu102\/libtorch-win-shared-with-deps-debug-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cu102\/libtorch-win-shared-with-deps-debug-latest.zip<\/a>","stable,libtorch,windows,cpu,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cpu\/libtorch-win-shared-with-deps-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cpu\/libtorch-win-shared-with-deps-latest.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cpu\/libtorch-win-shared-with-deps-debug-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cpu\/libtorch-win-shared-with-deps-debug-latest.zip<\/a>","stable,pip,windows,cuda11.1,python":"pip3 install --pre torch torchvision torchaudio -f https:\/\/download.pytorch.org\/whl\/nightly\/cu111\/torch_nightly.html","stable,pip,windows,cuda10.2,python":"pip3 install --pre torch torchvision torchaudio -f https:\/\/download.pytorch.org\/whl\/nightly\/cu102\/torch_nightly.html","stable,libtorch,windows,cuda11.1,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cu111\/libtorch-win-shared-with-deps-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cu111\/libtorch-win-shared-with-deps-latest.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/nightly\/cu111\/libtorch-win-shared-with-deps-debug-latest.zip\">https:\/\/download.pytorch.org\/libtorch\/nightly\/cu111\/libtorch-win-shared-with-deps-debug-latest.zip<\/a>","lts,pip,linux,cpu,python":"pip3 install torch==1.8.2+cpu torchvision==0.9.2+cpu torchaudio==0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,pip,linux,cuda10.2,python":"pip3 install torch==1.8.2+cu102 torchvision==0.9.2+cu102 torchaudio==0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,pip,linux,cuda11.1,python":"pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio==0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,pip,linux,rocm,python":"NOTE: ROCm is not supported in LTS","lts,conda,linux,cuda10.2,python":"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-lts","lts,conda,linux,cuda11.1,python":"NOTE: \\'nvidia\\' channel is required for cudatoolkit 11.1\r\nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia","lts,conda,linux,rocm,python":"NOTE: ROCm is not supported in LTS","lts,conda,linux,cpu,python":"conda install pytorch torchvision torchaudio cpuonly -c pytorch-lts","lts,libtorch,linux,cpu,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-shared-with-deps-1.8.2%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-shared-with-deps-1.8.2%2Bcpu.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcpu.zip<\/a>","lts,libtorch,linux,cuda10.2,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-shared-with-deps-1.8.2%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-shared-with-deps-1.8.2%2Bcu102.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcu102.zip<\/a>","lts,libtorch,linux,cuda11.1,cplusplus":"Download here (Pre-cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-shared-with-deps-1.8.2%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-shared-with-deps-1.8.2%2Bcu111.zip<\/a>\r\nDownload here (cxx11 ABI):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-cxx11-abi-shared-with-deps-1.8.2%2Bcu111.zip<\/a>","lts,libtorch,linux,rocm,python":"NOTE: ROCm is not supported in LTS","lts,pip,mac,cuda10.2,python":"# macOS is not currently supported for lts","lts,pip,mac,cuda11.1,python":"# macOS is not currently supported for lts","lts,pip,mac,rocm,python":"# macOS is not currently supported for lts","lts,pip,windows,cpu,python":"pip3 install torch==1.8.2+cpu torchvision==0.9.2+cpu torchaudio===0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,pip,windows,cuda10.2,python":"pip3 install torch==1.8.2+cu102 torchvision==0.9.2+cu102 torchaudio===0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,pip,windows,cuda11.1,python":"pip3 install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https:\/\/download.pytorch.org\/whl\/lts\/1.8\/torch_lts.html","lts,conda,windows,cuda10.2,python":"conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-lts","lts,conda,windows,cuda11.1,python":"NOTE: \\'conda-forge\\' channel is required for cudatoolkit 11.1\r\nconda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c conda-forge","lts,conda,windows,cpu,python":"conda install pytorch torchvision torchaudio cpuonly -c pytorch-lts","lts,conda,windows,rocm,python":"NOTE: ROCm is not supported in LTS","lts,libtorch,windows,cpu,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-win-shared-with-deps-1.8.2%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-win-shared-with-deps-1.8.2%2Bcpu.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcpu.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cpu\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcpu.zip<\/a>","lts,libtorch,windows,cuda10.2,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-win-shared-with-deps-1.8.2%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-win-shared-with-deps-1.8.2%2Bcu102.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcu102.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu102\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcu102.zip<\/a>","lts,libtorch,windows,cuda11.1,cplusplus":"Download here (Release version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-win-shared-with-deps-1.8.2%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-win-shared-with-deps-1.8.2%2Bcu111.zip<\/a>\r\nDownload here (Debug version):\r\n<a href=\"https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcu111.zip\">https:\/\/download.pytorch.org\/libtorch\/lts\/1.8\/cu111\/libtorch-win-shared-with-deps-debug-1.8.2%2Bcu111.zip<\/a>"};
</script>
<script src="/wp-content/themes/pytorch/assets/js/index.bundle.js?ver=1" id="pytorch-classic-js-js"></script>
<script src="/wp-content/themes/pytorch/assets/js/blog-header.bundle.js?ver=1" id="pytorch-blog-header-js"></script>
<script src="/wp-content/themes/pytorch/assets/js/text.bundle.js?ver=1" id="pytorch-text-js"></script>
<script src="/wp-content/themes/pytorch/assets/js/youtube-video.bundle.js?ver=1" id="pytorch-youtube-video-js"></script>
<script src="/wp-content/themes/pytorch/assets/js/post-feed.bundle.js?ver=1" id="pytorch-post-feed-js"></script>
<script src="/wp-content/plugins/simply-static-pro/assets/fuse.js?ver=1.1" id="ssp-fuse-js"></script>
<script src="/wp-content/plugins/simply-static-pro/assets/ssp-search.js?ver=1.1" id="ssp-search-js"></script>
<script src="https://stats.wp.com/e-202203.js" defer></script>
<script>
	_stq = window._stq || [];
	_stq.push([ 'view', {v:'ext',j:'1:10.4',blog:'195752808',post:'311',tz:'0',srv:'pytorch-org-preprod.go-vip.net'} ]);
	_stq.push([ 'clickTrackerInit', '195752808', '311' ]);
</script>

</body>
</html>